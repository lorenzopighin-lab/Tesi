#!/usr/bin/env python3
"""
Kriging delle precipitazioni giornaliere con PyKrige su griglia allineata al DTM (30 m).

Prerequisiti:
- pykrige, numpy, pandas, rasterio, pyproj, tqdm (opzionale)

Struttura attesa dati:
1) Cartella con 28 file CSV, uno per stazione, con colonne almeno: ["date", "precipitation_mean"].
   - La colonna "date" deve essere interpretabile da pandas.to_datetime.
   - Il nome file (senza estensione) viene usato come station_id (adatta se necessario).
2) File CSV metadati stazioni con colonne: [station_id, lon, lat] (EPSG:4326).
3) Un DTM/DSM/DEM GeoTIFF (30 m) che definisce estensione, risoluzione e CRS della griglia output.

Output:
- Una GeoTIFF per ogni data (una banda), con il kriging della precipitazione; stessa georeferenziazione del DTM.
- Facoltativo: puoi adattare per scrivere un multi-banda o NetCDF/Xarray.

Note importanti:
- Il kriging lavora meglio in coordinate proiettate (metri). Qui proiettiamo le stazioni nel CRS del DTM.
- I parametri del variogramma sono lasciati automatici (fit interno). Per lavori seri, fornisci parametri espliciti.
"""
from __future__ import annotations

import os
import glob
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple
import numpy as np
import pandas as pd
import rasterio
from rasterio.transform import Affine
from rasterio.transform import rowcol
from pyproj import Transformer

from pykrige.ok import OrdinaryKriging

try:
    from tqdm import tqdm
except Exception:  # tqdm è opzionale
    def tqdm(x, **kwargs):
        return x

# ==============================
# Configurazione (MODIFICA QUI)
# ==============================
CSV_DIR = Path("C:/Users/loren/Desktop/Tesi_magi/codes/data/DEA12030")              # cartella con i 28 csv
STATION_META = Path("C:/Users/loren/Desktop/Tesi_magi/codes/data/gauge_coord.csv")    # station_id,lon,lat (EPSG:4326)
DTM_PATH = Path("C:/Users/loren/Desktop/Tesi_magi/codes/data/DE12030.tif")              # raster di riferimento (griglia/CRS)
OUT_DIR = Path("C:/Users/loren/Desktop/Tesi_magi/codes/kriging_out")
OUT_PREFIX = "precip_"                         # prefisso file di output

# Impostazioni kriging
VARIOGRAM_MODEL = "exponential"                  # "spherical","exponential","gaussian","power","linear"
VARIOGRAM_PARAMS = None                        # None -> fit automatico; oppure dict/tuple con parametri
N_LAGS = 6                                     # usato dal fit automatico
ENABLE_STATS = False

# Filtro dati (facoltativo)
MIN_STATIONS_PER_DAY = 5                       # esegui kriging solo se >= N stazioni disponibili quel giorno
FILL_MISSING = False                            # se True, riempi buchi giornalieri per stazione con NaN (reindex)

# Parametri default per il calcolo della serie media per catchment
BUFFER_DISTANCE_METERS = 20000.0
MAX_STATIONS_PER_CATCHMENT: Optional[int] = 12
MIN_STATIONS_PER_CATCHMENT = 3
WEIGHT_DISTANCE_EPS = 100.0
INSIDE_WEIGHT_BONUS = 1.5


# ==============================
# Utility
# ==============================

def read_station_metadata(meta_path: Path) -> pd.DataFrame:
    df = pd.read_csv(meta_path)
    required = {"station_id", "lon", "lat"}
    if not required.issubset(df.columns):
        raise ValueError(f"Il file metadata deve contenere colonne {required}. Trovate: {set(df.columns)}")
    return df


def read_station_csvs(csv_dir: Path) -> pd.DataFrame:
    """Legge tutti i CSV in cartella e ritorna un DataFrame long: [date, station_id, precipitation_mean]."""
    rows = []
    for fp in sorted(glob.glob(str(csv_dir / "*.csv"))):
        station_id = Path(fp).stem  # adatta qui se il mapping station_id <- file differisce
        df = pd.read_csv(fp)
        if "date" not in df.columns or "precipitation_mean" not in df.columns:
            raise ValueError(f"'{fp}' deve contenere colonne 'date' e 'precipitation_mean'.")
        df["date"] = pd.to_datetime(df["date"], utc=True).dt.tz_convert(None)
        df = df[["date", "precipitation_mean"]].copy()
        df["station_id"] = station_id
        rows.append(df)
    all_df = pd.concat(rows, ignore_index=True)
    return all_df


def pivot_timeseries(long_df: pd.DataFrame, fill_missing: bool = False) -> pd.DataFrame:
    """Pivota a matrice tempo x stazione. Indice datetime giornaliero, colonne station_id."""
    wide = long_df.pivot_table(index="date", columns="station_id", values="precipitation_mean", aggfunc="mean")
    wide = wide.sort_index()
    if fill_missing:
        # Frequenza giornaliera continua
        full_idx = pd.date_range(wide.index.min().normalize(), wide.index.max().normalize(), freq="D")
        wide = wide.reindex(full_idx)
    return wide


def grid_from_dtm(dtm_path: Path) -> Tuple[np.ndarray, np.ndarray, Affine, dict, str]:
    """Estrae assi x/y (centri pixel) dal DTM, insieme a transform/profile/CRS."""
    with rasterio.open(dtm_path) as src:
        transform: Affine = src.transform
        width, height = src.width, src.height
        profile = src.profile
        crs = src.crs.to_string() if src.crs else None

    # Coordinate centri pixel
    x0, y0 = transform * (0, 0)
    dx, dy = transform.a, transform.e  # dy tipicamente negativo (north-up)

    xs = x0 + dx * (np.arange(width) + 0.5)
    ys = y0 + dy * (np.arange(height) + 0.5)

    # PyKrige 'grid' richiede assi crescenti: adeguiamo y se necessario
    y_ascending = ys if np.all(np.diff(ys) > 0) else ys[::-1]
    return xs, y_ascending, transform, profile, crs


def project_lonlat_to_crs(lon: np.ndarray, lat: np.ndarray, dst_crs: str) -> Tuple[np.ndarray, np.ndarray]:
    transformer = Transformer.from_crs("EPSG:4326", dst_crs, always_xy=True)
    x, y = transformer.transform(lon, lat)
    return np.asarray(x), np.asarray(y)


def krige_one_day(
    date: pd.Timestamp,
    values_by_station: pd.Series,
    station_xy: pd.DataFrame,
    gridx: np.ndarray,
    gridy: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray]:
    """Esegue Ordinary Kriging per un singolo giorno su griglia (gridx, gridy)."""
    # Stazioni disponibili quel giorno
    day_vals = values_by_station.dropna()
    st_ids = day_vals.index.tolist()
    if len(st_ids) < MIN_STATIONS_PER_DAY:
        raise RuntimeError(f"{date.date()}: stazioni disponibili {len(st_ids)} < {MIN_STATIONS_PER_DAY}")

    xs = station_xy.loc[st_ids, "x"].to_numpy()
    ys = station_xy.loc[st_ids, "y"].to_numpy()
    zs = day_vals.to_numpy()

 # Se tutti i valori disponibili sono identici, il fit automatico del variogramma
    # di PyKrige fallisce (semivarianza nulla -> bound sovrapposti). In questi casi
    # possiamo restituire direttamente un campo costante evitando di invocare il solver.
    if zs.size and np.allclose(zs, zs[0], atol=1e-10, rtol=0):
        const_field = np.full((len(gridy), len(gridx)), zs[0], dtype=float)
        return const_field, np.zeros_like(const_field)

    OK = OrdinaryKriging(
        xs,
        ys,
        zs,
        variogram_model=VARIOGRAM_MODEL,
        variogram_parameters=VARIOGRAM_PARAMS,
        nlags=N_LAGS,
        enable_statistics=ENABLE_STATS,
        coordinates_type="euclidean",  # perché lavoriamo nel CRS del DTM (metri)
    )

    z_pred, z_var = OK.execute("grid", gridx, gridy)
    return np.asarray(z_pred), np.asarray(z_var)


def save_geotiff(out_path: Path, data2d: np.ndarray, profile: dict, transform: Affine):
    prof = profile.copy()
    prof.update(
        dtype="float32",
        count=1,
        compress="deflate",
        predictor=2,
        tiled=True,
        blockxsize=min(512, prof.get("width", 512)),
        blockysize=min(512, prof.get("height", 512)),
        nodata=-9999.0,
        transform=transform,
    )
    arr = data2d.astype("float32", copy=False)
    # sostituisci eventuali NaN
    arr = np.where(np.isfinite(arr), arr, prof["nodata"])
    with rasterio.open(out_path, "w", **prof) as dst:
        dst.write(arr, 1)


# ==============================
# Catchment helper structures
# ==============================


@dataclass
class CatchmentContext:
    catchment_id: str
    mask: np.ndarray
    cell_coords: np.ndarray
    boundary_coords: np.ndarray
    station_ids: List[str]
    station_xy: np.ndarray
    station_weights: np.ndarray


def _mask_to_coords(mask: np.ndarray, transform: Affine) -> np.ndarray:
    rows, cols = np.nonzero(mask)
    if rows.size == 0:
        return np.empty((0, 2), dtype=float)
    xs, ys = rasterio.transform.xy(transform, rows, cols, offset="center")
    return np.column_stack([np.asarray(xs, dtype=float), np.asarray(ys, dtype=float)])


def _compute_boundary_coords(mask: np.ndarray, transform: Affine) -> np.ndarray:
    if mask is None or not mask.any():
        return np.empty((0, 2), dtype=float)
    h, w = mask.shape
    boundary = np.zeros_like(mask, dtype=bool)
    rows, cols = np.nonzero(mask)
    for r, c in zip(rows, cols):
        r0 = max(r - 1, 0)
        r1 = min(r + 2, h)
        c0 = max(c - 1, 0)
        c1 = min(c + 2, w)
        if not mask[r0:r1, c0:c1].all():
            boundary[r, c] = True
    return _mask_to_coords(boundary, transform)


def _distance_to_coords(point_xy: Sequence[float], coords: np.ndarray) -> float:
    if coords.size == 0:
        return float("inf")
    diffs = coords - np.asarray(point_xy, dtype=float)
    dists = np.hypot(diffs[:, 0], diffs[:, 1])
    return float(dists.min())


def _station_inside_mask(point_xy: Sequence[float], transform: Affine, mask: np.ndarray) -> bool:
    try:
        row, col = rowcol(transform, point_xy[0], point_xy[1])
    except Exception:
        return False
    h, w = mask.shape
    if 0 <= row < h and 0 <= col < w:
        return bool(mask[row, col])
    return False


def _prepare_catchment_contexts(
    catchment_masks: Sequence[np.ndarray],
    transform: Affine,
    station_xy: pd.DataFrame,
    catchment_ids: Optional[Sequence[str]] = None,
    buffer_distance: float = BUFFER_DISTANCE_METERS,
    max_stations: Optional[int] = MAX_STATIONS_PER_CATCHMENT,
    weight_eps: float = WEIGHT_DISTANCE_EPS,
    inside_bonus: float = INSIDE_WEIGHT_BONUS,
) -> Tuple[Dict[str, CatchmentContext], List[str]]:
    contexts: Dict[str, CatchmentContext] = {}
    missing: List[str] = []

    for idx, mask in enumerate(catchment_masks):
        catchment_id = (
            catchment_ids[idx]
            if catchment_ids is not None and idx < len(catchment_ids)
            else f"catchment_{idx + 1:03d}"
        )

        if mask is None or not np.any(mask):
            missing.append(catchment_id)
            continue

        cell_coords = _mask_to_coords(mask, transform)
        if cell_coords.size == 0:
            missing.append(catchment_id)
            continue

        boundary_coords = _compute_boundary_coords(mask, transform)

        station_candidates = []
        for station_id, row in station_xy.iterrows():
            point_xy = (row["x"], row["y"])
            dist_center = _distance_to_coords(point_xy, cell_coords)
            if not np.isfinite(dist_center) or dist_center > buffer_distance:
                continue

            dist_boundary = _distance_to_coords(point_xy, boundary_coords)
            if not np.isfinite(dist_boundary):
                dist_boundary = dist_center

            weight = 1.0 / (dist_boundary + weight_eps)
            if _station_inside_mask(point_xy, transform, mask):
                weight *= inside_bonus

            station_candidates.append((station_id, point_xy, weight))

        if not station_candidates:
            missing.append(catchment_id)
            continue

        station_candidates.sort(key=lambda item: item[2], reverse=True)
        if max_stations is not None:
            station_candidates = station_candidates[:max_stations]

        station_ids = [item[0] for item in station_candidates]
        coords = np.asarray([item[1] for item in station_candidates], dtype=float)
        weights = np.asarray([item[2] for item in station_candidates], dtype=float)
        if weights.sum() > 0:
            weights = weights / weights.sum()

        contexts[catchment_id] = CatchmentContext(
            catchment_id=catchment_id,
            mask=np.asarray(mask, dtype=bool),
            cell_coords=cell_coords,
            boundary_coords=boundary_coords,
            station_ids=station_ids,
            station_xy=coords,
            station_weights=weights,
        )

    return contexts, missing


def _krige_catchment_day(
    date: pd.Timestamp,
    day_values: pd.Series,
    context: CatchmentContext,
    variogram_model: str,
    variogram_params,
    nlags: int,
    min_stations: int,
) -> float:
    if context.cell_coords.size == 0:
        return float("nan")

    values = day_values.reindex(context.station_ids)
    mask_valid = values.notna().to_numpy()
    if mask_valid.sum() < max(1, min_stations):
        return float("nan")

    xs = context.station_xy[mask_valid, 0]
    ys = context.station_xy[mask_valid, 1]
    zs = values.to_numpy()[mask_valid]

    if zs.size == 0:
        return float("nan")

    if np.allclose(zs, zs[0], atol=1e-10, rtol=0):
        return float(zs[0])

    try:
        OK = OrdinaryKriging(
            xs,
            ys,
            zs,
            variogram_model=variogram_model,
            variogram_parameters=variogram_params,
            nlags=nlags,
            enable_statistics=ENABLE_STATS,
            coordinates_type="euclidean",
        )

        preds, _ = OK.execute(
            "points",
            context.cell_coords[:, 0],
            context.cell_coords[:, 1],
        )
        preds = np.asarray(preds)
        if np.ma.isMaskedArray(preds):
            preds = preds.filled(np.nan)
        if preds.size == 0:
            return float("nan")
        return float(np.nanmean(preds))
    except Exception:
        weights = context.station_weights[mask_valid]
        if weights.size == 0 or not np.isfinite(weights).any():
            return float(np.nan)
        if weights.sum() > 0:
            weights = weights / weights.sum()
        print(
            f"[WARN] {date.date()}: kriging fallito per il bacino "
            f"{context.catchment_id}, uso media pesata."
        )
        return float(np.dot(weights, zs))


def compute_catchment_precipitation_series(
    catchment_masks: Sequence[np.ndarray],
    transform: Affine,
    catchment_ids: Optional[Sequence[str]] = None,
    selected_catchment_ids: Optional[Sequence[str]] = None,
    csv_dir: Path = CSV_DIR,
    station_meta: Path = STATION_META,
    dtm_crs: Optional[str] = None,
    start_date: str = "2000-01-01",
    end_date: str = "2020-12-31",
    buffer_distance: float = BUFFER_DISTANCE_METERS,
    min_stations: int = MIN_STATIONS_PER_CATCHMENT,
    max_stations: Optional[int] = MAX_STATIONS_PER_CATCHMENT,
    weight_eps: float = WEIGHT_DISTANCE_EPS,
    inside_bonus: float = INSIDE_WEIGHT_BONUS,
    fill_missing: bool = FILL_MISSING,
    variogram_model: str = VARIOGRAM_MODEL,
    variogram_params=VARIOGRAM_PARAMS,
    nlags: int = N_LAGS,
) -> pd.DataFrame:
    
    def _normalize_requested_ids(raw_ids: Optional[Sequence[str]]) -> List[str]:
        if not raw_ids:
            return []
        normalized: List[str] = []
        for item in raw_ids:
            text = str(item).strip()
            if not text:
                continue
            normalized.append(text)
        return normalized

    requested_ids = _normalize_requested_ids(selected_catchment_ids)

    if catchment_ids is None:
        catchment_pairs = [
            (f"catchment_{idx + 1:03d}", mask)
            for idx, mask in enumerate(catchment_masks)
        ]
    else:
        catchment_pairs = list(zip(catchment_ids, catchment_masks))

    if requested_ids:
        available_map = {cid: mask for cid, mask in catchment_pairs}
        filtered_pairs: List[Tuple[str, np.ndarray]] = []
        missing_requested: List[str] = []
        for cid in requested_ids:
            mask = available_map.get(cid)
            if mask is None:
                missing_requested.append(cid)
                continue
            filtered_pairs.append((cid, mask))

        if missing_requested:
            print(
                "Catchment richiesti non trovati per il kriging: "
                + ", ".join(missing_requested)
            )

        catchment_pairs = filtered_pairs

    if not catchment_pairs:
        return pd.DataFrame(columns=["date"])
    
    catchment_ids = [cid for cid, _ in catchment_pairs]
    catchment_masks = [mask for _, mask in catchment_pairs]

    csv_dir = Path(csv_dir)
    station_meta = Path(station_meta)

    meta = read_station_metadata(station_meta)
    ts_long = read_station_csvs(csv_dir)
    ts_wide = pivot_timeseries(ts_long, fill_missing=fill_missing)

    date_index = pd.date_range(start=start_date, end=end_date, freq="D")
    ts_wide = ts_wide.reindex(date_index)

    if dtm_crs is None:
        _, _, _, _, dtm_crs = grid_from_dtm(DTM_PATH)
    if dtm_crs is None:
        raise ValueError("Il DTM deve avere un CRS definito per proiettare le stazioni.")

    x, y = project_lonlat_to_crs(meta["lon"].to_numpy(), meta["lat"].to_numpy(), dtm_crs)
    meta = meta.set_index("station_id").copy()
    meta["x"], meta["y"] = x, y

    common = meta.index.intersection(ts_wide.columns)
    if common.empty:
        df = pd.DataFrame(index=date_index)
        df.index.name = "date"
        return df.reset_index()

    ts_wide = ts_wide[common]
    station_xy = meta.loc[common, ["x", "y"]]

    contexts, missing = _prepare_catchment_contexts(
        catchment_masks=catchment_masks,
        transform=transform,
        station_xy=station_xy,
        catchment_ids=catchment_ids,
        buffer_distance=buffer_distance,
        max_stations=max_stations,
        weight_eps=weight_eps,
        inside_bonus=inside_bonus,
    )

    if catchment_ids is None:
        all_ids = [f"catchment_{i + 1:03d}" for i in range(len(catchment_masks))]
    else:
        all_ids = list(catchment_ids)

    results = {cid: [] for cid in all_ids}
    catchment_durations = {cid: 0.0 for cid in all_ids}

    for date in date_index:
        day_values = ts_wide.loc[date]
        for cid in all_ids:
            context = contexts.get(cid)
            if context is None:
                results[cid].append(np.nan)
                continue
            start_time = time.perf_counter()
            value = _krige_catchment_day(
                date=date,
                day_values=day_values,
                context=context,
                variogram_model=variogram_model,
                variogram_params=variogram_params,
                nlags=nlags,
                min_stations=min_stations,
            )
            catchment_durations[cid] += time.perf_counter() - start_time
            results[cid].append(value)

    df = pd.DataFrame(results, index=date_index)
    df.index.name = "date"
    df = df.reset_index()
    
    for cid, duration in catchment_durations.items():
        if cid not in contexts:
            continue
        print(
            f"Interpolazione completata per il bacino {cid} in "
            f"{duration:.2f} secondi"
        )

    if missing:
        missing_str = ", ".join(missing)
        print(
            "Catchment senza stazioni sufficienti per il kriging: "
            f"{missing_str}"
        )

    return df


# ==============================
# Main workflow
# ==============================

def main():
    OUT_DIR.mkdir(parents=True, exist_ok=True)

    # 1) Dati stazioni
    meta = read_station_metadata(STATION_META)
    ts_long = read_station_csvs(CSV_DIR)

    # 2) Tabella wide: righe=date, colonne=station_id
    ts_wide = pivot_timeseries(ts_long, fill_missing=FILL_MISSING)

    # 3) Griglia dal DTM
    gridx, gridy, transform, profile, dtm_crs = grid_from_dtm(DTM_PATH)
    if dtm_crs is None:
        raise ValueError("Il DTM deve avere un CRS definito.")

    # 4) Proiezione stazioni nel CRS del DTM
    #    (assume meta lon/lat in EPSG:4326)
    x, y = project_lonlat_to_crs(meta["lon"].to_numpy(), meta["lat"].to_numpy(), dtm_crs)
    meta = meta.set_index("station_id").copy()
    meta["x"], meta["y"] = x, y

    # 5) Allinea elenco stazioni tra metadati e serie
    common = meta.index.intersection(ts_wide.columns)
    if len(common) < MIN_STATIONS_PER_DAY:
        raise ValueError("Troppe poche stazioni in comune tra metadati e CSV.")
    ts_wide = ts_wide[common]
    station_xy = meta.loc[common, ["x", "y"]]

    # 6) Loop giornaliero
    dates = ts_wide.index
    print(f"Date totali: {len(dates)}; stazioni: {len(common)}; griglia: {len(gridy)}x{len(gridx)}")

    # Nota: gridy passato a PyKrige è ascendente; per salvare GeoTIFF serve l'ordine del DTM (top->bottom).
    # Se il DTM ha dy negativo (classico north-up), ys originali erano decrescenti e abbiamo invertito per PyKrige.
    # Per scrivere, invertiamo di nuovo.
    dtm_dy = transform.e
    flip_y_for_write = dtm_dy < 0 and (np.diff(gridy)[0] > 0)

    for date in tqdm(dates, desc="Kriging giornaliero"):
        try:
            z_pred, _ = krige_one_day(date, ts_wide.loc[date], station_xy, gridx, gridy)
            if flip_y_for_write:
                z_pred = z_pred[::-1, :]  # torna all'ordine a righe del DTM
            out_name = f"{OUT_PREFIX}{pd.Timestamp(date).strftime('%Y%m%d')}.tif"
            save_geotiff(OUT_DIR / out_name, z_pred, profile, transform)
        except RuntimeError as e:
            # troppo poche stazioni o altri problemi: salta il giorno
            print(f"SKIP {date.date()}: {e}")

    print("Fatto.")


if __name__ == "__main__":
    main()
