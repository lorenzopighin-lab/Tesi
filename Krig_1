#!/usr/bin/env python3
"""
Kriging delle precipitazioni giornaliere con PyKrige su griglia allineata al DTM (30 m).

Prerequisiti:
- pykrige, numpy, pandas, rasterio, pyproj, tqdm (opzionale)

Struttura attesa dati:
11) Cartella con un file per stazione (formato .txt) contenente le precipitazioni.
   - Il nome file (es. "15.txt") corrisponde all'ID della stazione.
   - Separatore di colonna: punto e virgola (;)
   - Colonne attese: "MESS_DATUM" (formato data YYYYMMDD) e "RS" (precipitazione).
2) File CSV metadati stazioni con colonne: [ID, Lon, Lat] (EPSG:4326).
   - In alternativa sono accettate le intestazioni [station_id, lon, lat].
3) Un DTM/DSM/DEM GeoTIFF (30 m) che definisce estensione, risoluzione e CRS della griglia output.

Output:
- Una GeoTIFF per ogni data (una banda), con il kriging della precipitazione; stessa georeferenziazione del DTM.
- Facoltativo: puoi adattare per scrivere un multi-banda o NetCDF/Xarray.

Note importanti:
- Il kriging lavora meglio in coordinate proiettate (metri). Qui proiettiamo le stazioni nel CRS del DTM.
- I parametri del variogramma sono lasciati automatici (fit interno). Per lavori seri, fornisci parametri espliciti.
"""
from __future__ import annotations

import glob
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple
import numpy as np
import pandas as pd
import rasterio
from rasterio.transform import Affine, rowcol
from pyproj import Transformer

from pykrige.ok import OrdinaryKriging

try:
    from tqdm import tqdm
except Exception:  # tqdm è opzionale
    def tqdm(x, **kwargs):
        return x

# ==============================
# Configurazione (MODIFICA QUI)
# ==============================
CSV_DIR = Path("C:/Users/loren/Desktop/Tesi_magi/codes/data/pluv/precip_DE911970")              # cartella con i file .txt delle precipitazioni
STATION_META = Path("C:/Users/loren/Desktop/Tesi_magi/codes/data/pluv/pluv_DE911970.csv")    # CSV con ID/Lon/Lat (EPSG:4326)
DTM_PATH = Path("C:/Users/loren/Desktop/Tesi_magi/codes/data/DE12030.tif")              # raster di riferimento (griglia/CRS)
OUT_DIR = Path("C:/Users/loren/Desktop/Tesi_magi/codes/kriging_out")
OUT_PREFIX = "precip_"                         # prefisso file di output

# Impostazioni kriging
VARIOGRAM_MODEL = "exponential"                  # "spherical","exponential","gaussian","power","linear"
VARIOGRAM_PARAMS = None                        # None -> fit automatico; oppure dict/tuple con parametri
N_LAGS = 6                                     # usato dal fit automatico
ENABLE_STATS = False

# Filtro dati (facoltativo)
MIN_STATIONS_PER_DAY = 5                       # esegui kriging solo se >= N stazioni disponibili quel giorno
FILL_MISSING = False                            # se True, riempi buchi giornalieri per stazione con NaN (reindex)

# Parametri default per il calcolo della serie media per catchment
BUFFER_DISTANCE_METERS = 20000.0
MAX_STATIONS_PER_CATCHMENT: Optional[int] = 12
MIN_STATIONS_PER_CATCHMENT = 3
WEIGHT_DISTANCE_EPS = 100.0
INSIDE_WEIGHT_BONUS = 1
PRECIPITATION_THRESHOLD = 0.0


# ==============================
# Utility
# ==============================

def read_station_metadata(meta_path: Path) -> pd.DataFrame:
    df = pd.read_csv(meta_path)
    df.columns = [str(col).strip() for col in df.columns]
    columns_lower = {col.lower(): col for col in df.columns}
    if {"station_id", "lon", "lat"}.issubset(columns_lower):
        rename_map = {
            columns_lower["station_id"]: "station_id",
            columns_lower["lon"]: "lon",
            columns_lower["lat"]: "lat",
        }
    elif {"id", "lon", "lat"}.issubset(columns_lower):
        rename_map = {
            columns_lower["id"]: "station_id",
            columns_lower["lon"]: "lon",
            columns_lower["lat"]: "lat",
        }
    else:
        raise ValueError(
            "Il file metadata deve contenere colonne [ID, Lon, Lat] "
            "oppure [station_id, lon, lat]. "
            f"Trovate: {set(df.columns)}"
        )
    df = df.rename(columns=rename_map)
    return df[["station_id", "lon", "lat"]]

def _parse_date_column(date_series: pd.Series) -> pd.Series:
    """Converte una colonna data gestendo formati giorno/mese e avvisa se ci sono valori ignorati."""

    first_pass = pd.to_datetime(date_series, utc=True, errors="coerce")
    if first_pass.isna().any():
        # Alcuni dataset storici usano il formato giorno/mese/anno: prova a interpretarlo
        dayfirst_pass = pd.to_datetime(date_series, utc=True, dayfirst=True, errors="coerce")
        if dayfirst_pass.notna().sum() > first_pass.notna().sum():
            parsed = dayfirst_pass
        else:
            parsed = first_pass
    else:
        parsed = first_pass

    invalid_count = int(parsed.isna().sum())
    parsed = parsed.dt.tz_convert(None)

    if invalid_count:
        print(f"Avviso: {invalid_count} righe con date non interpretabili sono state scartate.")

    return parsed

def _prepare_station_timeseries(
    df: pd.DataFrame,
    station_id: str,
    *,
    date_col: str,
    precip_col: str,
    date_format: str | None = None,
) -> pd.DataFrame:
    if date_format:
        parsed = pd.to_datetime(df[date_col].astype(str), format=date_format, errors="coerce")
        parsed = parsed.dt.tz_localize(None)
    else:
        parsed = _parse_date_column(df[date_col])
    df = df.assign(date=parsed)
    df = df.dropna(subset=["date"])
    if df.empty:
        raise ValueError(
            f"'{station_id}' non contiene date interpretabili. Verifica il formato."
        )
    precip = pd.to_numeric(df[precip_col], errors="coerce")
    out = pd.DataFrame(
        {
            "date": df["date"],
            "precipitation_mean": precip,
            "station_id": station_id,
        }
    )
    return out.dropna(subset=["precipitation_mean"])



def read_station_csvs(csv_dir: Path) -> pd.DataFrame:
    """Legge i file di precipitazione in cartella e ritorna un DataFrame long."""
    rows = []
    files = sorted(glob.glob(str(csv_dir / "*.txt"))) + sorted(glob.glob(str(csv_dir / "*.csv")))
    if not files:
        raise ValueError(f"Nessun file .txt o .csv trovato in {csv_dir}")
    for fp in files:
        station_id = Path(fp).stem  # il nome file coincide con l'ID della stazione
        if fp.lower().endswith(".txt"):
            df = pd.read_csv(fp, sep=";", dtype=str)
        else:
            df = pd.read_csv(fp)
        df.columns = [str(col).strip() for col in df.columns]
        columns_lower = {col.lower(): col for col in df.columns}
        if {"mess_datum", "rs"}.issubset(columns_lower):
            df = _prepare_station_timeseries(
                df,
                station_id,
                date_col=columns_lower["mess_datum"],
                precip_col=columns_lower["rs"],
                date_format="%Y%m%d",
            )
        elif {"date", "precipitation_mean"}.issubset(columns_lower):
            df = _prepare_station_timeseries(
                df,
                station_id,
                date_col=columns_lower["date"],
                precip_col=columns_lower["precipitation_mean"],
            )
        else:
            raise ValueError(
                f"'{fp}' deve contenere colonne 'MESS_DATUM'/'RS' "
                "o 'date'/'precipitation_mean'."
            )
        rows.append(df)
    all_df = pd.concat(rows, ignore_index=True)
    return all_df


def pivot_timeseries(long_df: pd.DataFrame, fill_missing: bool = False) -> pd.DataFrame:
    """Pivota a matrice tempo x stazione. Indice datetime giornaliero, colonne station_id."""
    wide = long_df.pivot_table(index="date", columns="station_id", values="precipitation_mean", aggfunc="mean")
    wide = wide.sort_index()
    if fill_missing:
        # Frequenza giornaliera continua
        full_idx = pd.date_range(wide.index.min().normalize(), wide.index.max().normalize(), freq="D")
        wide = wide.reindex(full_idx)
    return wide


def grid_from_dtm(dtm_path: Path) -> Tuple[np.ndarray, np.ndarray, Affine, dict, str]:
    """Estrae assi x/y (centri pixel) dal DTM, insieme a transform/profile/CRS."""
    with rasterio.open(dtm_path) as src:
        transform: Affine = src.transform
        width, height = src.width, src.height
        profile = src.profile
        crs = src.crs.to_string() if src.crs else None

    # Coordinate centri pixel
    x0, y0 = transform * (0, 0)
    dx, dy = transform.a, transform.e  # dy tipicamente negativo (north-up)

    xs = x0 + dx * (np.arange(width) + 0.5)
    ys = y0 + dy * (np.arange(height) + 0.5)

    # PyKrige 'grid' richiede assi crescenti: adeguiamo y se necessario
    y_ascending = ys if np.all(np.diff(ys) > 0) else ys[::-1]
    return xs, y_ascending, transform, profile, crs


def project_lonlat_to_crs(lon: np.ndarray, lat: np.ndarray, dst_crs: str) -> Tuple[np.ndarray, np.ndarray]:
    transformer = Transformer.from_crs("EPSG:4326", dst_crs, always_xy=True)
    x, y = transformer.transform(lon, lat)
    return np.asarray(x), np.asarray(y)


def krige_one_day(
    date: pd.Timestamp,
    values_by_station: pd.Series,
    station_xy: pd.DataFrame,
    gridx: np.ndarray,
    gridy: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray]:
    """Esegue Ordinary Kriging per un singolo giorno su griglia (gridx, gridy)."""
    # Stazioni disponibili quel giorno
    day_vals = values_by_station.dropna()
    st_ids = day_vals.index.tolist()
    if len(st_ids) < MIN_STATIONS_PER_DAY:
        raise RuntimeError(f"{date.date()}: stazioni disponibili {len(st_ids)} < {MIN_STATIONS_PER_DAY}")

    xs = station_xy.loc[st_ids, "x"].to_numpy()
    ys = station_xy.loc[st_ids, "y"].to_numpy()
    zs = day_vals.to_numpy()

 # Se tutti i valori disponibili sono identici, il fit automatico del variogramma
    # di PyKrige fallisce (semivarianza nulla -> bound sovrapposti). In questi casi
    # possiamo restituire direttamente un campo costante evitando di invocare il solver.
    if zs.size and np.allclose(zs, zs[0], atol=1e-10, rtol=0):
        const_field = np.full((len(gridy), len(gridx)), zs[0], dtype=float)
        return const_field, np.zeros_like(const_field)

    OK = OrdinaryKriging(
        xs,
        ys,
        zs,
        variogram_model=VARIOGRAM_MODEL,
        variogram_parameters=VARIOGRAM_PARAMS,
        nlags=N_LAGS,
        enable_statistics=ENABLE_STATS,
        coordinates_type="euclidean",  # perché lavoriamo nel CRS del DTM (metri)
    )

    z_pred, z_var = OK.execute("grid", gridx, gridy)
    return np.asarray(z_pred), np.asarray(z_var)


def save_geotiff(out_path: Path, data2d: np.ndarray, profile: dict, transform: Affine):
    prof = profile.copy()
    prof.update(
        dtype="float32",
        count=1,
        compress="deflate",
        predictor=2,
        tiled=True,
        blockxsize=min(512, prof.get("width", 512)),
        blockysize=min(512, prof.get("height", 512)),
        nodata=-9999.0,
        transform=transform,
    )
    arr = data2d.astype("float32", copy=False)
    # sostituisci eventuali NaN
    arr = np.where(np.isfinite(arr), arr, prof["nodata"])
    with rasterio.open(out_path, "w", **prof) as dst:
        dst.write(arr, 1)


# ==============================
# Catchment helper structures
# ==============================


@dataclass
class CatchmentContext:
    catchment_id: str
    mask: np.ndarray
    cell_coords: np.ndarray
    boundary_coords: np.ndarray
    station_ids: List[str]
    station_xy: np.ndarray
    station_weights: np.ndarray


def _mask_to_coords(mask: np.ndarray, transform: Affine) -> np.ndarray:
    rows, cols = np.nonzero(mask)
    if rows.size == 0:
        return np.empty((0, 2), dtype=float)
    xs, ys = rasterio.transform.xy(transform, rows, cols, offset="center")
    return np.column_stack([np.asarray(xs, dtype=float), np.asarray(ys, dtype=float)])

def aggregate_mask_to_blocks(
    mask: np.ndarray, transform: Affine, factor: int = 3
) -> Tuple[np.ndarray, Affine]:
    """Aggregate a boolean mask into larger square blocks.

    The function groups pixels in ``factor x factor`` blocks (default 3x3) and
    marks the aggregated pixel as ``True`` if at least one of the original
    pixels is ``True``. The output transform is scaled accordingly so that the
    georeferencing remains consistent with the coarser grid.
    """

    if factor <= 1:
        return np.asarray(mask, dtype=bool), transform

    mask = np.asarray(mask, dtype=bool)
    height, width = mask.shape
    new_h = (height // factor) * factor
    new_w = (width // factor) * factor

    if new_h == 0 or new_w == 0:
        return np.zeros((0, 0), dtype=bool), transform * Affine.scale(factor)

    trimmed = mask[:new_h, :new_w]
    reshaped = trimmed.reshape(new_h // factor, factor, new_w // factor, factor)
    aggregated = reshaped.any(axis=(1, 3))
    new_transform = transform * Affine.scale(factor)
    return aggregated, new_transform

def _compute_boundary_coords(mask: np.ndarray, transform: Affine) -> np.ndarray:
    if mask is None or not mask.any():
        return np.empty((0, 2), dtype=float)
    h, w = mask.shape
    boundary = np.zeros_like(mask, dtype=bool)
    rows, cols = np.nonzero(mask)
    for r, c in zip(rows, cols):
        r0 = max(r - 1, 0)
        r1 = min(r + 2, h)
        c0 = max(c - 1, 0)
        c1 = min(c + 2, w)
        if not mask[r0:r1, c0:c1].all():
            boundary[r, c] = True
    return _mask_to_coords(boundary, transform)


def _distance_to_coords(point_xy: Sequence[float], coords: np.ndarray) -> float:
    if coords.size == 0:
        return float("inf")
    diffs = coords - np.asarray(point_xy, dtype=float)
    dists = np.hypot(diffs[:, 0], diffs[:, 1])
    return float(dists.min())


def _station_inside_mask(point_xy: Sequence[float], transform: Affine, mask: np.ndarray) -> bool:
    try:
        row, col = rowcol(transform, point_xy[0], point_xy[1])
    except Exception:
        return False
    h, w = mask.shape
    if 0 <= row < h and 0 <= col < w:
        return bool(mask[row, col])
    return False


def _prepare_catchment_contexts(
    catchment_masks: Sequence[np.ndarray],
    transform: Affine,
    station_xy: pd.DataFrame,
    catchment_ids: Optional[Sequence[str]] = None,
    buffer_distance: float = BUFFER_DISTANCE_METERS,
    max_stations: Optional[int] = MAX_STATIONS_PER_CATCHMENT,
    weight_eps: float = WEIGHT_DISTANCE_EPS,
    inside_bonus: float = INSIDE_WEIGHT_BONUS,
) -> Tuple[Dict[str, CatchmentContext], List[str]]:
    contexts: Dict[str, CatchmentContext] = {}
    missing: List[str] = []

    for idx, mask in enumerate(catchment_masks):
        catchment_id = (
            catchment_ids[idx]
            if catchment_ids is not None and idx < len(catchment_ids)
            else f"catchment_{idx + 1:03d}"
        )

        if mask is None or not np.any(mask):
            missing.append(catchment_id)
            continue

        cell_coords = _mask_to_coords(mask, transform)
        if cell_coords.size == 0:
            missing.append(catchment_id)
            continue

        boundary_coords = _compute_boundary_coords(mask, transform)

        station_candidates = []
        for station_id, row in station_xy.iterrows():
            point_xy = (row["x"], row["y"])
            dist_center = _distance_to_coords(point_xy, cell_coords)
            if not np.isfinite(dist_center) or dist_center > buffer_distance:
                continue

            dist_boundary = _distance_to_coords(point_xy, boundary_coords)
            if not np.isfinite(dist_boundary):
                dist_boundary = dist_center

            weight = 1.0 / (dist_boundary + weight_eps)
            if _station_inside_mask(point_xy, transform, mask):
                weight *= inside_bonus

            station_candidates.append((station_id, point_xy, weight))

        if not station_candidates:
            missing.append(catchment_id)
            continue

        station_candidates.sort(key=lambda item: item[2], reverse=True)
        if max_stations is not None:
            station_candidates = station_candidates[:max_stations]

        station_ids = [item[0] for item in station_candidates]
        coords = np.asarray([item[1] for item in station_candidates], dtype=float)
        weights = np.asarray([item[2] for item in station_candidates], dtype=float)
        if weights.sum() > 0:
            weights = weights / weights.sum()

        contexts[catchment_id] = CatchmentContext(
            catchment_id=catchment_id,
            mask=np.asarray(mask, dtype=bool),
            cell_coords=cell_coords,
            boundary_coords=boundary_coords,
            station_ids=station_ids,
            station_xy=coords,
            station_weights=weights,
        )

    return contexts, missing


def _krige_catchment_day(
    date: pd.Timestamp,
    day_values: pd.Series,
    context: CatchmentContext,
    variogram_model: str,
    variogram_params,
    nlags: int,
    min_stations: int,
    return_predictions: bool = False,
):
    if context.cell_coords.size == 0:
        return float("nan")

    values = day_values.reindex(context.station_ids)
    mask_valid = values.notna().to_numpy()
    if mask_valid.sum() < max(1, min_stations):
        return float("nan")

    xs = context.station_xy[mask_valid, 0]
    ys = context.station_xy[mask_valid, 1]
    zs = values.to_numpy()[mask_valid]

    if zs.size == 0:
        return float("nan")

    if np.allclose(zs, zs[0], atol=1e-10, rtol=0):
        const_val = float(zs[0])
        if return_predictions:
            return np.full(context.cell_coords.shape[0], const_val, dtype=float)
        return const_val

    try:
        OK = OrdinaryKriging(
            xs,
            ys,
            zs,
            variogram_model=variogram_model,
            variogram_parameters=variogram_params,
            nlags=nlags,
            enable_statistics=ENABLE_STATS,
            coordinates_type="euclidean",
        )

        preds, _ = OK.execute(
            "points",
            context.cell_coords[:, 0],
            context.cell_coords[:, 1],
        )
        preds = np.asarray(preds)
        if np.ma.isMaskedArray(preds):
            preds = preds.filled(np.nan)
        if preds.size == 0:
            return float("nan") if not return_predictions else preds
        if return_predictions:
            return preds
        return float(np.nanmean(preds))
    except Exception:
        weights = context.station_weights[mask_valid]
        if weights.size == 0 or not np.isfinite(weights).any():
            fallback = float(np.nan)
            return (
                np.full(context.cell_coords.shape[0], fallback)
                if return_predictions
                else fallback
            )
        if weights.sum() > 0:
            weights = weights / weights.sum()
        print(
            f"[WARN] {date.date()}: kriging fallito per il bacino "
            f"{context.catchment_id}, uso media pesata."
        )
        fallback = float(np.dot(weights, zs))
        return (
            np.full(context.cell_coords.shape[0], fallback)
            if return_predictions
            else fallback
        )


def compute_catchment_precipitation_series(
    catchment_masks: Sequence[np.ndarray],
    transform: Affine,
    catchment_ids: Optional[Sequence[str]] = None,
    selected_catchment_ids: Optional[Sequence[str]] = None,
    csv_dir: Path = CSV_DIR,
    station_meta: Path = STATION_META,
    dtm_path: Optional[Path] = None,
    dtm_crs: Optional[str] = None,
    start_date: str = "2000-01-01",
    end_date: str = "2020-12-31",
    buffer_distance: float = BUFFER_DISTANCE_METERS,
    min_stations: int = MIN_STATIONS_PER_CATCHMENT,
    max_stations: Optional[int] = MAX_STATIONS_PER_CATCHMENT,
    weight_eps: float = WEIGHT_DISTANCE_EPS,
    inside_bonus: float = INSIDE_WEIGHT_BONUS,
    fill_missing: bool = FILL_MISSING,
    variogram_model: str = VARIOGRAM_MODEL,
    variogram_params=VARIOGRAM_PARAMS,
    nlags: int = N_LAGS,
    aggregation_factor: int = 3,
    precipitation_threshold: float = PRECIPITATION_THRESHOLD,
) -> pd.DataFrame:
    
    def _normalize_requested_ids(raw_ids: Optional[Sequence[str]]) -> List[str]:
        if not raw_ids:
            return []
        normalized: List[str] = []
        for item in raw_ids:
            text = str(item).strip()
            if not text:
                continue
            normalized.append(text)
        return normalized

    requested_ids = _normalize_requested_ids(selected_catchment_ids)

    if catchment_ids is None:
        catchment_pairs = [
            (f"catchment_{idx + 1:03d}", mask)
            for idx, mask in enumerate(catchment_masks)
        ]
    else:
        catchment_pairs = list(zip(catchment_ids, catchment_masks))

    if requested_ids:
        available_map = {cid: mask for cid, mask in catchment_pairs}
        filtered_pairs: List[Tuple[str, np.ndarray]] = []
        missing_requested: List[str] = []
        for cid in requested_ids:
            mask = available_map.get(cid)
            if mask is None:
                missing_requested.append(cid)
                continue
            filtered_pairs.append((cid, mask))

        if missing_requested:
            print(
                "Catchment richiesti non trovati per il kriging: "
                + ", ".join(missing_requested)
            )

        catchment_pairs = filtered_pairs

    if not catchment_pairs:
        return pd.DataFrame(columns=["date"])
    
    catchment_ids = [cid for cid, _ in catchment_pairs]
    catchment_masks = [mask for _, mask in catchment_pairs]

    csv_dir = Path(csv_dir)
    station_meta = Path(station_meta)
    dtm_path = Path(dtm_path) if dtm_path is not None else DTM_PATH
    
    meta = read_station_metadata(station_meta)
    ts_long = read_station_csvs(csv_dir)
    ts_wide = pivot_timeseries(ts_long, fill_missing=fill_missing)

    date_index = pd.date_range(start=start_date, end=end_date, freq="D")
    ts_wide = ts_wide.reindex(date_index)

    if dtm_crs is None:
        _, _, _, _, dtm_crs = grid_from_dtm(dtm_path)
    if dtm_crs is None:
        raise ValueError("Il DTM deve avere un CRS definito per proiettare le stazioni.")

    x, y = project_lonlat_to_crs(meta["lon"].to_numpy(), meta["lat"].to_numpy(), dtm_crs)
    meta = meta.set_index("station_id").copy()
    meta["x"], meta["y"] = x, y

    common = meta.index.intersection(ts_wide.columns)
    if common.empty:
        df = pd.DataFrame(index=date_index)
        df.index.name = "date"
        return df.reset_index()

    ts_wide = ts_wide[common]
    station_xy = meta.loc[common, ["x", "y"]]

    catchment_masks = [np.asarray(mask, dtype=bool) for mask in catchment_masks]
    agg_transform = transform
    if aggregation_factor and aggregation_factor > 1:
        aggregated_masks = []
        for mask in catchment_masks:
            aggregated, agg_transform = aggregate_mask_to_blocks(
                mask, transform, aggregation_factor
            )
            aggregated_masks.append(aggregated)
        catchment_masks = aggregated_masks

    basin_mask = np.logical_or.reduce(catchment_masks)
    if basin_mask is None or not basin_mask.any():
        return pd.DataFrame(columns=["date"])

    basin_rows, basin_cols = np.nonzero(basin_mask)

    basin_contexts, missing = _prepare_catchment_contexts(
        catchment_masks=[basin_mask],
        transform=agg_transform,
        station_xy=station_xy,
        catchment_ids=["basin"],
        buffer_distance=buffer_distance,
        max_stations=max_stations,
        weight_eps=weight_eps,
        inside_bonus=inside_bonus,
    )
    basin_context = basin_contexts.get("basin")
    if basin_context is None:
        return pd.DataFrame(columns=["date"])

    if catchment_ids is None:
        all_ids = [f"catchment_{i + 1:03d}" for i in range(len(catchment_masks))]
    else:
        all_ids = list(catchment_ids)

    results = {cid: [] for cid in all_ids}
    catchment_durations = {cid: 0.0 for cid in all_ids}

    for date in date_index:
        day_values = ts_wide.loc[date]
        start_time = time.perf_counter()
        basin_preds = _krige_catchment_day(
            date=date,
            day_values=day_values,
            context=basin_context,
            variogram_model=variogram_model,
            variogram_params=variogram_params,
            nlags=nlags,
            min_stations=min_stations,
            return_predictions=True,
        )

        basin_grid = np.full(basin_mask.shape, np.nan)
        if isinstance(basin_preds, np.ndarray):
            basin_grid[basin_rows, basin_cols] = basin_preds
        else:
            basin_grid[basin_rows, basin_cols] = basin_preds

        elapsed = time.perf_counter() - start_time
        for cid, mask in zip(all_ids, catchment_masks):
            if mask is None or not np.any(mask):
                results[cid].append(np.nan)
                continue
            values = basin_grid[mask]
            mean_precip = float(np.nanmean(values))
            if np.isnan(mean_precip):
                results[cid].append(mean_precip)
                continue
            if mean_precip < precipitation_threshold:
                mean_precip = 0.0
            results[cid].append(mean_precip)
            catchment_durations[cid] += elapsed

    df = pd.DataFrame(results, index=date_index)
    df.index.name = "date"
    df = df.reset_index()
    
    # Arrotonda le colonne numeriche a due decimali per garantire al massimo
    # due cifre dopo la virgola nelle serie di precipitazione in output.
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        df[numeric_cols] = df[numeric_cols].round(2)
    
    for cid, duration in catchment_durations.items():
        print(
            f"Interpolazione completata per il bacino {cid} in "
            f"{duration:.2f} secondi"
        )

    if missing:
        missing_str = ", ".join(missing)
        print(
            "Catchment senza stazioni sufficienti per il kriging: "
            f"{missing_str}"
        )

    return df

# ==============================
# Basin-wide kriging helper
# ==============================


def krige_basin_precipitation_grid(
    date: str,
    catchment_masks: Sequence[np.ndarray],
    transform: Affine,
    csv_dir: Path = CSV_DIR,
    station_meta: Path = STATION_META,
    dtm_path: Optional[Path] = None,
    dtm_crs: Optional[str] = None,
    buffer_distance: float = BUFFER_DISTANCE_METERS,
    max_stations: Optional[int] = MAX_STATIONS_PER_CATCHMENT,
    weight_eps: float = WEIGHT_DISTANCE_EPS,
    inside_bonus: float = INSIDE_WEIGHT_BONUS,
    variogram_model: str = VARIOGRAM_MODEL,
    variogram_params=VARIOGRAM_PARAMS,
    nlags: int = N_LAGS,
    aggregation_factor: int = 3,
) -> Tuple[np.ndarray, Affine, pd.DataFrame]:
    """Esegue il kriging su tutto il bacino (unione dei catchment) per una data."""

    target_date = pd.to_datetime(date)
    if pd.isna(target_date):
        raise ValueError(f"Data non valida per il kriging: {date}")

    csv_dir = Path(csv_dir)
    station_meta = Path(station_meta)
    dtm_path = Path(dtm_path) if dtm_path is not None else DTM_PATH

    meta = read_station_metadata(station_meta)
    ts_long = read_station_csvs(csv_dir)
    ts_wide = pivot_timeseries(ts_long, fill_missing=FILL_MISSING)

    ts_wide = ts_wide.reindex(pd.DatetimeIndex([target_date]))

    if dtm_crs is None:
        _, _, _, _, dtm_crs = grid_from_dtm(dtm_path)
    if dtm_crs is None:
        raise ValueError("Il DTM deve avere un CRS definito per proiettare le stazioni.")

    x, y = project_lonlat_to_crs(meta["lon"].to_numpy(), meta["lat"].to_numpy(), dtm_crs)
    meta = meta.set_index("station_id").copy()
    meta["x"], meta["y"] = x, y

    common = meta.index.intersection(ts_wide.columns)
    if common.empty:
        raise ValueError("Nessuna stazione comune tra metadati e serie di precipitazione.")

    ts_wide = ts_wide[common]
    station_xy = meta.loc[common, ["x", "y"]]

    catchment_masks = [np.asarray(mask, dtype=bool) for mask in catchment_masks]
    agg_transform = transform
    if aggregation_factor and aggregation_factor > 1:
        aggregated_masks = []
        for mask in catchment_masks:
            aggregated, agg_transform = aggregate_mask_to_blocks(
                mask, transform, aggregation_factor
            )
            aggregated_masks.append(aggregated)
        catchment_masks = aggregated_masks

    basin_mask = np.logical_or.reduce(catchment_masks)
    if basin_mask is None or not basin_mask.any():
        raise ValueError("Il bacino aggregato non contiene pixel validi.")
        
    station_data = meta.loc[common, ["lon", "lat", "x", "y"]].copy()
    station_data["precipitation"] = ts_wide.loc[target_date].reindex(common).to_numpy()

    station_data["inside_basin"] = [
        _station_inside_mask((row["x"], row["y"]), agg_transform, basin_mask)
        for _, row in station_data.iterrows()
    ]

    basin_rows, basin_cols = np.nonzero(basin_mask)

    basin_contexts, missing = _prepare_catchment_contexts(
        catchment_masks=[basin_mask],
        transform=agg_transform,
        station_xy=station_xy,
        catchment_ids=["basin"],
        buffer_distance=buffer_distance,
        max_stations=max_stations,
        weight_eps=weight_eps,
        inside_bonus=inside_bonus,
    )

    if missing:
        raise ValueError(
            "Stazioni insufficienti nel buffer del bacino per eseguire il kriging."
        )

    basin_context = basin_contexts.get("basin")
    basin_preds = _krige_catchment_day(
        date=target_date,
        day_values=ts_wide.loc[target_date],
        context=basin_context,
        variogram_model=variogram_model,
        variogram_params=variogram_params,
        nlags=nlags,
        min_stations=MIN_STATIONS_PER_CATCHMENT,
        return_predictions=True,
    )

    basin_grid = np.full(basin_mask.shape, np.nan)
    basin_grid[basin_rows, basin_cols] = basin_preds
    return basin_grid, agg_transform, station_data
# ==============================
# Main workflow
# ==============================

def main():
    OUT_DIR.mkdir(parents=True, exist_ok=True)

    # 1) Dati stazioni
    meta = read_station_metadata(STATION_META)
    ts_long = read_station_csvs(CSV_DIR)

    # 2) Tabella wide: righe=date, colonne=station_id
    ts_wide = pivot_timeseries(ts_long, fill_missing=FILL_MISSING)

    # 3) Griglia dal DTM
    gridx, gridy, transform, profile, dtm_crs = grid_from_dtm(DTM_PATH)
    if dtm_crs is None:
        raise ValueError("Il DTM deve avere un CRS definito.")

    # 4) Proiezione stazioni nel CRS del DTM
    #    (assume meta lon/lat in EPSG:4326)
    x, y = project_lonlat_to_crs(meta["lon"].to_numpy(), meta["lat"].to_numpy(), dtm_crs)
    meta = meta.set_index("station_id").copy()
    meta["x"], meta["y"] = x, y

    # 5) Allinea elenco stazioni tra metadati e serie
    common = meta.index.intersection(ts_wide.columns)
    if len(common) < MIN_STATIONS_PER_DAY:
        raise ValueError("Troppe poche stazioni in comune tra metadati e CSV.")
    ts_wide = ts_wide[common]
    station_xy = meta.loc[common, ["x", "y"]]

    # 6) Loop giornaliero
    dates = ts_wide.index
    print(f"Date totali: {len(dates)}; stazioni: {len(common)}; griglia: {len(gridy)}x{len(gridx)}")

    # Nota: gridy passato a PyKrige è ascendente; per salvare GeoTIFF serve l'ordine del DTM (top->bottom).
    # Se il DTM ha dy negativo (classico north-up), ys originali erano decrescenti e abbiamo invertito per PyKrige.
    # Per scrivere, invertiamo di nuovo.
    dtm_dy = transform.e
    flip_y_for_write = dtm_dy < 0 and (np.diff(gridy)[0] > 0)

    for date in tqdm(dates, desc="Kriging giornaliero"):
        try:
            z_pred, _ = krige_one_day(date, ts_wide.loc[date], station_xy, gridx, gridy)
            if flip_y_for_write:
                z_pred = z_pred[::-1, :]  # torna all'ordine a righe del DTM
            out_name = f"{OUT_PREFIX}{pd.Timestamp(date).strftime('%Y%m%d')}.tif"
            save_geotiff(OUT_DIR / out_name, z_pred, profile, transform)
        except RuntimeError as e:
            # troppo poche stazioni o altri problemi: salta il giorno
            print(f"SKIP {date.date()}: {e}")

    print("Fatto.")


if __name__ == "__main__":
    main()
