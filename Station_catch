# -*- coding: utf-8 -*-
"""
Created on Wed Nov 26 16:09:52 2025

@author: loren
"""
import os
import math
import csv
import array 
import matplotlib.colors as colors
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar
import numpy as np
import pandas as pd
import rasterio
import seaborn as sns
from pathlib import Path
from pysheds.grid import Grid
from rasterio.transform import rowcol
from Krig_2 import (
    compute_catchment_precipitation_series,
    krige_basin_precipitation_grid,
)
from Model_2 import run_model



# --- Configurazione generale ---

dem_path  = "C:/Users/loren/Desktop/Tesi_magi/codes/data/DE111140.tif"
POUR_POINT = array.array('f', [9.596098,	47.627986])
SNAP_ACCUMULATION_THRESHOLD = 9000
BRANCH_ACCUMULATION_THRESHOLD = 8000
MIN_ACCUMULATION_KM2 = 10


# --- Parametri dimensionali ---
# Il DEM è espresso in coordinate geografiche ma sappiamo che le celle
# rappresentano quadrati da 30 m di lato.  Per le operazioni che richiedono
# distanze o superfici in metri utilizziamo quindi questo fattore di
# conversione esplicito.
PIXEL_SIZE_METERS = 30.0
PIXEL_AREA_M2 = PIXEL_SIZE_METERS ** 2

# --- Parametri selezione pixel ---
# "equidistant": distribuisce i pixel intermedi in modo equidistante lungo il ramo
# "spacing": posiziona i pixel intermedi ogni INTERMEDIATE_SPACING_METERS lungo il ramo
# Sui rami più corti di MIN_BRANCH_LENGTH_FOR_INTERMEDIATE_METERS non vengono
# selezionati pixel intermedi.
PIXEL_PLACEMENT_MODE = "spacing"
MAX_INTERMEDIATE_PIXELS = 2
INTERMEDIATE_SPACING_METERS = 600.0
MIN_BRANCH_LENGTH_FOR_INTERMEDIATE_METERS = 5000
MAIN_SEED_EXCLUSION_RADIUS_METERS = 5000.0
TARGET_STATION_ID = ""

# --- Esportazione tabella pixel selezionati ---
PIXEL_TABLE_OUTPUT = Path("selected_pixels_table.csv")

# --- Configurazione piogge medie per catchment ---
# Stazioni attuali usate come sezioni di chiusura.
STATION_META_PATH = Path("C:/Users/loren/Desktop/Tesi_magi/codes/data/gauge_coord_DE111140.csv")
# Inserire qui il path alla cartella con i file .txt delle precipitazioni (uno per stazione).
PRECIP_DATA_DIR = Path("C:/Users/loren/Desktop/Tesi_magi/codes/data/pluv/precip_DE111140")
# Inserire qui il path al CSV con le colonne ID, Lon, Lat delle stazioni di precipitazione.
PRECIP_STATION_META_PATH = Path("C:/Users/loren/Desktop/Tesi_magi/codes/data/pluv/pluv_DE111140.csv")
PRECIP_DATE_START = "2000-01-01"
PRECIP_DATE_END = "2020-12-31"
PRECIP_BUFFER_METERS = 20000.0
PRECIP_MIN_STATIONS = 2
PRECIP_MAX_STATIONS = 30
AGGREGATION_FACTOR = 3  # raggruppa i pixel del DEM in blocchi quadrati da n x n
KRIGING_PRECIPITATION_THRESHOLD = 0.0  # nessuna soglia durante l'interpolazione
MODEL_PRECIPITATION_THRESHOLDS = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]  # soglie da applicare quando si lancia Model_1
CATCHMENT_SERIES_OUTPUT = Path("C:/Users/loren/Desktop/Tesi_magi/codes/kriging_out/arg_exp_catchment_precipitation.csv")
MODEL_OUTPUT_DIR = Path("C:/Users/loren/Desktop/Tesi_magi/codes/model_out/DE11140/model_2_linear")


# --- Opzioni Kriging ---
RUN_KRIGING = True  # Imposta a False per saltare il calcolo delle precipitazioni
PROMPT_KRIGING_CATCHMENTS = True  # Chiedi da input gli ID dei catchment da interpolare
# Esempio: ["catchment_001", "catchment_010"] oppure stringa "catchment_001,catchment_010"
KRIGING_CATCHMENT_IDS = None
PROMPT_PLOT_PRECIPITATION = True
PLOT_PRECIPITATION_DATE = None  # es. "2015-10-10" per plottare le piogge sul bacino principale
PLOT_PRECIPITATION_RANGE = ("2015-10-01", "2015-10-15")  # es. ("2015-10-01", "2015-10-05") per plottare più giorni consecutivi

def _format_scalebar_label(length_meters: float) -> str:
    if length_meters >= 1000:
        km_value = length_meters / 1000.0
        if math.isclose(km_value, round(km_value), rel_tol=1e-9):
            return f"{int(round(km_value))} km"
        return f"{km_value:.1f} km"
    if math.isclose(length_meters, round(length_meters), rel_tol=1e-9):
        return f"{int(round(length_meters))} m"
    return f"{length_meters:.0f} m"


def add_scale_bar(
    ax,
    transform,
    *,
    preferred_lengths_meters=None,
    location="lower right",
    color="black",
    pixel_size_meters: float = PIXEL_SIZE_METERS,
):
    """Add a simple scale bar to ``ax`` based on the raster transform."""
    if transform is None:
        return
    try:
        pixel_width_units = abs(float(transform.a))
    except (AttributeError, TypeError, ValueError):
        pixel_width_units = None
    if not pixel_width_units:
        return
    if not pixel_size_meters or pixel_size_meters <= 0:
        return

    meters_per_unit = pixel_size_meters / pixel_width_units
    if not np.isfinite(meters_per_unit) or meters_per_unit <= 0:
        return

    xlim = ax.get_xlim()
    axis_width_units = abs(xlim[1] - xlim[0])
    axis_width_meters = axis_width_units * meters_per_unit
    if axis_width_meters <= 0:
        return

    if not preferred_lengths_meters:
        preferred_lengths_meters = [50, 100, 200, 500, 1000, 2000, 5000, 10000]

    selected_length = None
    for candidate in preferred_lengths_meters:
        if candidate <= axis_width_meters * 0.4:
            selected_length = candidate
    if selected_length is None:
        selected_length = axis_width_meters / 5
    if selected_length <= 0 or not np.isfinite(selected_length):
        return

    length_data_units = selected_length / meters_per_unit
    if not np.isfinite(length_data_units) or length_data_units <= 0:
        return

    fontprops = fm.FontProperties(size=10)
    size_vertical = max(length_data_units * 0.05, axis_width_units * 0.002)
    scalebar = AnchoredSizeBar(
        ax.transData,
        length_data_units,
        _format_scalebar_label(selected_length),
        loc=location,
        pad=0.4,
        borderpad=0.4,
        sep=4,
        color=color,
        frameon=True,
        size_vertical=size_vertical,
        fontproperties=fontprops,
    )
    ax.add_artist(scalebar)

def _extent_from_transform(transform, width: int, height: int):
    x_min = transform.c
    x_max = x_min + transform.a * width
    y_max = transform.f
    y_min = y_max + transform.e * height
    return [x_min, x_max, y_min, y_max]


def plot_precipitation_map(
    date_text: str,
    grid_data: np.ndarray,
    transform,
    station_data: pd.DataFrame | None = None,
    *,
    pixel_size_meters: float = PIXEL_SIZE_METERS,
):
    """Crea un plot con i pixel colorati in base alle precipitazioni e le stazioni misurate."""

    if grid_data.size == 0:
        print("Nessun dato di precipitazione da plottare.")
        return

    height, width = grid_data.shape
    extent = _extent_from_transform(transform, width, height)

    all_values = []
    if np.isfinite(grid_data).any():
        all_values.append(float(np.nanmax(grid_data)))

    if station_data is not None and not station_data.empty:
        station_vals = station_data.get("precipitation")
        if station_vals is not None:
            finite_station = station_vals[np.isfinite(station_vals)]
            if finite_station.size:
                all_values.append(float(np.nanmax(finite_station)))

    vmax = max(all_values) if all_values else 0.0
    vmin = 0.0
    if vmax <= vmin:
        vmax = vmin + 1.0
        
    cmap = plt.get_cmap("Blues").copy()
    cmap.set_bad(color="white", alpha=0.0)
    norm = colors.Normalize(vmin=vmin, vmax=vmax)

    fig, ax = plt.subplots(figsize=(10, 8))
    img = ax.imshow(
        np.ma.masked_invalid(grid_data),
        origin="upper",
        extent=extent,
        cmap=cmap,
        norm=norm,
    )

    basin_mask = np.isfinite(grid_data)
    if basin_mask.any():
        x_coords = np.linspace(extent[0], extent[1], width)
        y_coords = np.linspace(extent[3], extent[2], height)
        ax.contour(
            x_coords,
            y_coords,
            basin_mask,
            levels=[0.5],
            colors="black",
            linewidths=1.4,
            zorder=5,
        )
        
    if station_data is not None and not station_data.empty:
        finite_precip = station_data["precipitation"].to_numpy()
        point_colors = []
        for val in finite_precip:
            if np.isfinite(val):
                point_colors.append(cmap(norm(val)))
            else:
                point_colors.append("#e53935")

        ax.scatter(
            station_data["lon"],
            station_data["lat"],
            s=55,
            marker="o",
            facecolor=point_colors,
            edgecolor="black",
            linewidth=0.6,
            zorder=6,
            label="Stazioni gauge",
        )
        _annotate_station_labels(ax, station_data)

    ax.set_xlabel("x")
    ax.set_ylabel("y")
    ax.set_title(f"Precipitazione interpolata {date_text}")
    add_scale_bar(ax, transform, pixel_size_meters=pixel_size_meters)
    cbar = fig.colorbar(img, ax=ax, label="Precipitazione [mm]")
    cbar.ax.tick_params(labelsize=10)
    plt.tight_layout()
    plt.show()

def _load_station_points(meta_path: Path):
    if not meta_path:
        return [], None, None
    try:
        with meta_path.open(newline="", encoding="utf-8") as fp:
            reader = csv.DictReader(fp)
            required_fields = {"lon", "lat", "station_id"}
            if not reader.fieldnames or required_fields - set(reader.fieldnames):
                missing = ", ".join(sorted(required_fields - set(reader.fieldnames or [])))
                print(
                    "Impossibile tracciare le stazioni: il file di metadata non contiene "
                    f"le colonne richieste ({missing})."
                )
                return [], None, None
            records = []
            lons = []
            lats = []
            for row in reader:
                try:
                    lon = float(row["lon"])
                    lat = float(row["lat"])
                except (TypeError, ValueError):
                    continue
                if not (math.isfinite(lon) and math.isfinite(lat)):
                    continue
                station_id = (row.get("station_id") or "").strip()
                if not station_id:
                    continue
                record = {
                    "station_id": station_id,
                    "lon": float(lon),
                    "lat": float(lat),
                    "raw": row,
                }
                records.append(record)
                lons.append(lon)
                lats.append(lat)
    except FileNotFoundError:
        print(f"File metadata stazioni non trovato: {meta_path}")
        return [], None, None
    except OSError as exc:
        print(f"Impossibile leggere il file metadata delle stazioni '{meta_path}': {exc}")
        return [], None, None

    if not records:
        print("Nessuna stazione valida trovata nel file di metadata delle stazioni.")
        return [], None, None
    return (
        records,
        np.asarray(lons, dtype="float32"),
        np.asarray(lats, dtype="float32"),
    )


station_metadata, station_lons, station_lats = _load_station_points(STATION_META_PATH)

def _load_precip_station_points(meta_path: Path):
    if not meta_path:
        return [], None, None
    try:
        with meta_path.open(newline="", encoding="utf-8") as fp:
            reader = csv.DictReader(fp)
            if not reader.fieldnames:
                print("Impossibile tracciare le stazioni: file metadata vuoto.")
                return [], None, None
            field_map = {name.lower(): name for name in reader.fieldnames}
            required_fields = {"id", "lon", "lat"}
            if required_fields - set(field_map):
                missing = ", ".join(sorted(required_fields - set(field_map)))
                print(
                    "Impossibile tracciare le stazioni di precipitazione: "
                    f"mancano le colonne richieste ({missing})."
                )
                return [], None, None
            records = []
            lons = []
            lats = []
            for row in reader:
                try:
                    lon = float(row[field_map["lon"]])
                    lat = float(row[field_map["lat"]])
                except (TypeError, ValueError):
                    continue
                if not (math.isfinite(lon) and math.isfinite(lat)):
                    continue
                station_id = (row.get(field_map["id"]) or "").strip()
                if not station_id:
                    continue
                record = {
                    "station_id": station_id,
                    "lon": float(lon),
                    "lat": float(lat),
                    "raw": row,
                }
                records.append(record)
                lons.append(lon)
                lats.append(lat)
    except FileNotFoundError:
        print(f"File metadata stazioni di precipitazione non trovato: {meta_path}")
        return [], None, None
    except OSError as exc:
        print(
            "Impossibile leggere il file metadata delle stazioni di precipitazione "
            f"'{meta_path}': {exc}"
        )
        return [], None, None

    if not records:
        print("Nessuna stazione di precipitazione valida trovata nel file metadata.")
        return [], None, None
    return (
        records,
        np.asarray(lons, dtype="float32"),
        np.asarray(lats, dtype="float32"),
    )


precip_station_metadata, precip_station_lons, precip_station_lats = (
    _load_precip_station_points(PRECIP_STATION_META_PATH)
)

def _annotate_station_labels(ax, records, *, text_color="black", fontsize=8):
    """Annota i nomi delle stazioni su un asse Matplotlib.

    Parameters
    ----------
    ax : matplotlib.axes.Axes
        Asse su cui disegnare le etichette.
    records : list[dict]
        Lista di record delle stazioni, ciascuno con chiavi ``station_id``,
        ``lon`` e ``lat``.
    text_color : str, optional
        Colore del testo, di default "black".
    fontsize : int or float, optional
        Dimensione del font delle etichette, di default 8.
    """

    if records is None:
        return

    if isinstance(records, pd.DataFrame):
        if records.empty:
            return
        records = records.to_dict("records")
    elif not records:
        return

    renderer = None
    placed_bboxes = []
    candidate_offsets = [
        (4, 4),
        (-4, 4),
        (4, -4),
        (-4, -4),
        (8, 0),
        (-8, 0),
        (0, 8),
        (0, -8),
        (10, 6),
        (-10, 6),
        (10, -6),
        (-10, -6),
    ]

    def _create_annotation(lon, lat, text, offset):
        return ax.annotate(
            text,
            (lon, lat),
            xytext=offset,
            textcoords="offset points",
            fontsize=fontsize,
            color=text_color,
            bbox={"boxstyle": "round,pad=0.2", "facecolor": "white", "alpha": 0.7},
        )
    
    for rec in records:
        lon = rec.get("lon")
        lat = rec.get("lat")
        label = (rec.get("station_id") or "").strip()
        if not label or lon is None or lat is None:
            continue

        if renderer is None:
            ax.figure.canvas.draw()
            renderer = ax.figure.canvas.get_renderer()

        text_obj = None
        bbox = None
        for offset in candidate_offsets:
            text_obj = _create_annotation(lon, lat, label, offset)
            bbox = text_obj.get_window_extent(renderer=renderer).expanded(1.08, 1.1)
            if any(bbox.overlaps(existing) for existing in placed_bboxes):
                text_obj.remove()
                text_obj = None
                continue
            placed_bboxes.append(bbox)
            break

        if text_obj is None:
            text_obj = _create_annotation(lon, lat, label, candidate_offsets[0])
            bbox = text_obj.get_window_extent(renderer=renderer).expanded(1.08, 1.1)
            placed_bboxes.append(bbox)


def _filter_stations_in_catchment(records, catch_mask, transform):
    if not records or catch_mask is None or transform is None:
        return []

    mask_array = np.asarray(catch_mask)
    if mask_array.ndim != 2:
        return []

    height, width = mask_array.shape
    inside_records = []

    for rec in records:
        lon = rec.get("lon")
        lat = rec.get("lat")
        if lon is None or lat is None:
            continue

        try:
            row, col = rowcol(transform, lon, lat)
        except Exception:
            continue

        if 0 <= row < height and 0 <= col < width and mask_array[row, col]:
            inside_records.append(rec)

    return inside_records

def _build_plot_date_list(
    date_string: str | None = None, date_range: tuple[str, str] | None = None
) -> list[str]:
    dates: list[str] = []

    if date_range and len(date_range) == 2:
        start_raw, end_raw = date_range
        start = pd.to_datetime(start_raw, errors="coerce")
        end = pd.to_datetime(end_raw, errors="coerce")
        if not (pd.isna(start) or pd.isna(end)):
            for ts in pd.date_range(start=start.normalize(), end=end.normalize(), freq="D"):
                dates.append(ts.strftime("%Y-%m-%d"))

    if date_string and not dates:
        ts = pd.to_datetime(date_string, errors="coerce")
        if not pd.isna(ts):
            dates.append(ts.strftime("%Y-%m-%d"))

    return dates
with rasterio.open(dem_path) as src:
    dem = src.read(1, out_dtype='float32')
    profile = src.profile

# --- Lettura dei dati di input ---
grid = Grid.from_raster(os.path.join(dem_path), nodata=-9999, data_name="grid data")
dem = grid.read_raster(os.path.join(dem_path), nodata=-9999, data_name="dem")

flooded_dem = grid.fill_depressions(dem)
inflated_dem = grid.resolve_flats(flooded_dem, eps=1e-10) #eps è importante
fdir = grid.flowdir(inflated_dem)

# Calcola l'accumulo di flusso a valle di ogni cella.
acc = grid.accumulation(fdir)

# Aggancia il punto di chiusura sul pixel con accumulo sufficiente.
x_snap, y_snap = grid.snap_to_mask(
    acc > SNAP_ACCUMULATION_THRESHOLD, POUR_POINT)


# --- Visualizzazione accumulo di flusso e punto di chiusura ---
fig, ax = plt.subplots(figsize=(8, 6))
fig.patch.set_alpha(0)
plt.grid('on', zorder=3)
im = ax.imshow(acc, extent=grid.extent, zorder=2,
               cmap='cubehelix',
               norm=colors.LogNorm(1, acc.max()),
               interpolation='bilinear')
ax.scatter([x_snap], [y_snap], s=80, facecolors='none', edgecolors='red',
           linewidth=1.8, zorder=4, label='Punto')

plt.colorbar(im, ax=ax, label='Upstream Cells')
plt.title('Flow Accumulation', size=14)
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.tight_layout()


# Delineate the catchment
catch = grid.catchment(x=x_snap, y=y_snap, fdir=fdir, xytype='coordinate')
stations_in_catchment = _filter_stations_in_catchment(
    station_metadata, catch, grid.affine
)
n_pixels = int(np.count_nonzero(catch))   # equivalente a int(catch.sum())
print("Pixel nel catchment:", n_pixels)
main_catchment_area_m2 = n_pixels * PIXEL_AREA_M2
main_catchment_area_km2 = main_catchment_area_m2 / 1e6
#%%%plots
# DEM
inflated_dem_masked = np.ma.masked_invalid(inflated_dem)

fig, ax = plt.subplots(figsize=(8, 6))
dem_im = ax.imshow(
    inflated_dem_masked,
    extent=grid.extent,
    cmap='terrain',
    origin='upper',
)
ax.scatter(
    [x_snap],
    [y_snap],
    s=80,
    facecolors='none',
    edgecolors='red',
    linewidth=1.8,
    label='Sezione di chiusura',
)
if (
    station_lons is not None
    and station_lats is not None
    and getattr(station_lons, "size", 0)
    and getattr(station_lats, "size", 0)
):
    ax.scatter(
        station_lons,
        station_lats,
        s=45,
        marker='^',
        facecolors='#f9c74f',
        edgecolors='black',
        linewidth=0.6,
        zorder=5,
        label='stazioni di misura',
    )
    if stations_in_catchment:
        _annotate_station_labels(ax, stations_in_catchment)
ax.set_title('DSM', size=14)
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
add_scale_bar(ax, grid.affine, location='lower right')
plt.colorbar(dem_im, ax=ax, label='Elevation')
ax.legend(loc='upper right')
plt.tight_layout()

# Catchment outline over DEM
fig, ax = plt.subplots(figsize=(8, 6))
dem_im = ax.imshow(
    inflated_dem_masked,
    extent=grid.extent,
    cmap='terrain',
    origin='upper',
)

# coordinate arrays for contour plotting
x_coords = np.linspace(grid.extent[0], grid.extent[1], catch.shape[1])
y_coords = np.linspace(grid.extent[3], grid.extent[2], catch.shape[0])

catch_bool = catch.astype(bool)
ax.contour(
    x_coords,
    y_coords,
    catch_bool,
    levels=[0.5],
    colors='dodgerblue',
    linewidths=1.8,
)

# optional filled overlay of the catchment (transparent red)
catch_overlay = np.ma.masked_where(~catch_bool, catch_bool)
ax.imshow(
    catch_overlay,
    extent=grid.extent,
    origin='upper',
    cmap=colors.ListedColormap([(1.0, 0.0, 0.0, 0.35)]),
    zorder=4,
)
ax.scatter(
    [x_snap],
    [y_snap],
    s=80,
    facecolors='none',
    edgecolors='red',
    linewidth=1.8,
    label='Punto',
)
if (
    station_lons is not None
    and station_lats is not None
    and getattr(station_lons, "size", 0)
    and getattr(station_lats, "size", 0)
):
    ax.scatter(
        station_lons,
        station_lats,
        s=45,
        marker='^',
        facecolors='#f9c74f',
        edgecolors='black',
        linewidth=0.6,
        zorder=5,
        label='stazioni di misura',
    )
    if stations_in_catchment:
        _annotate_station_labels(ax, stations_in_catchment)
ax.set_title('Catchment e stazioni di portata', size=14)
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
add_scale_bar(ax, grid.affine, location='lower right')
plt.colorbar(dem_im, ax=ax, label='Elevation')
plt.tight_layout()
plt.show()

# Catchment trasparente con stazioni di precipitazione
fig, ax = plt.subplots(figsize=(8, 6))
dem_im = ax.imshow(
    inflated_dem_masked,
    extent=grid.extent,
    cmap='terrain',
    origin='upper',
)
ax.contour(
    x_coords,
    y_coords,
    catch_bool,
    levels=[0.5],
    colors='dodgerblue',
    linewidths=1.8,
)
ax.imshow(
    catch_overlay,
    extent=grid.extent,
    origin='upper',
    cmap=colors.ListedColormap([(1.0, 0.0, 0.0, 0.35)]),
    zorder=4,
)
if (
    precip_station_lons is not None
    and precip_station_lats is not None
    and getattr(precip_station_lons, "size", 0)
    and getattr(precip_station_lats, "size", 0)
):
    ax.scatter(
        precip_station_lons,
        precip_station_lats,
        s=50,
        marker='o',
        facecolors='#90caf9',
        edgecolors='black',
        linewidth=0.6,
        zorder=6,
        label='stazioni precipitazione',
    )
    if precip_station_metadata:
        _annotate_station_labels(ax, precip_station_metadata)
ax.set_title('Catchment con stazioni di precipitazione', size=14)
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
add_scale_bar(ax, grid.affine, location='lower right')
plt.colorbar(dem_im, ax=ax, label='Elevation')
ax.legend(loc='upper right')
plt.tight_layout()

plt.show()

# Catchment con stazioni di precipitazione (senza etichette ID)
fig, ax = plt.subplots(figsize=(8, 6))
dem_im = ax.imshow(
    inflated_dem_masked,
    extent=grid.extent,
    cmap='terrain',
    origin='upper',
)
ax.contour(
    x_coords,
    y_coords,
    catch_bool,
    levels=[0.5],
    colors='dodgerblue',
    linewidths=1.8,
)
ax.imshow(
    catch_overlay,
    extent=grid.extent,
    origin='upper',
    cmap=colors.ListedColormap([(1.0, 0.0, 0.0, 0.35)]),
    zorder=4,
)
if (
    precip_station_lons is not None
    and precip_station_lats is not None
    and getattr(precip_station_lons, "size", 0)
    and getattr(precip_station_lats, "size", 0)
):
    ax.scatter(
        precip_station_lons,
        precip_station_lats,
        s=50,
        marker='o',
        facecolors='#90caf9',
        edgecolors='black',
        linewidth=0.6,
        zorder=6,
    )
ax.set_title('Catchment e stazioni di precipitazione', size=14)
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
add_scale_bar(ax, grid.affine, location='lower left')
plt.colorbar(dem_im, ax=ax, label='Elevation')
plt.tight_layout()
#%%branches
# Limita la griglia al bacino e ricava la rete drenante principale.
grid.clip_to(catch)
catch_view = grid.view(catch).astype(bool)
branches = grid.extract_river_network(
    fdir, acc > BRANCH_ACCUMULATION_THRESHOLD
)
                                      
# coordinate del seed principale nella view clippata
main_seed_row, main_seed_col = map(int, rowcol(grid.affine, x_snap, y_snap))
main_seed_x, main_seed_y = rasterio.transform.xy(
    grid.affine, main_seed_row, main_seed_col, offset="center")

#%%%plot branches 

sns.set_palette('husl')
fig, ax = plt.subplots(figsize=(8.5,6.5))

plt.xlim(grid.bbox[0], grid.bbox[2])
plt.ylim(grid.bbox[1], grid.bbox[3])
ax.set_aspect('equal')
add_scale_bar(ax, grid.affine, location='lower left')

for branch in branches['features']:
    line = np.asarray(branch['geometry']['coordinates'])
    plt.plot(line[:, 0], line[:, 1])
    
_ = plt.title(
    f"Channel network (>{BRANCH_ACCUMULATION_THRESHOLD} accumulation)",
    size=14,
)
#%% Impostazioni distribuzione pixel lungo i rami
# I parametri sono definiti nella sezione di configurazione iniziale.
# Rasterizza la rete per identificare i pixel appartenenti a ciascun ramo.

fdir_view = grid.view(fdir)
transform_view = grid.affine       # affine della view corrente (dopo il clip)
shape_view = fdir_view.shape

shapes = (
    (feat["geometry"], idx + 1) for idx, feat in enumerate(branches["features"])
)
branches_raster = rasterio.features.rasterize(
    shapes=shapes,
    out_shape=shape_view,
    transform=transform_view,
    fill=0,
    all_touched=False,
    dtype="uint16",
)


# --- identificazione delle confluenze e dei pixel a monte ---
acc_view = grid.view(acc)              # acc calcolato prima: acc = grid.accumulation(fdir)
H, W = acc_view.shape

cell_area_m2 = PIXEL_AREA_M2
cell_size_x = PIXEL_SIZE_METERS
cell_size_y = PIXEL_SIZE_METERS
thr_cells = math.ceil((MIN_ACCUMULATION_KM2 * 1e6) / cell_area_m2)

# Seleziona i pixel con accumulo sufficiente per essere considerati candidati.
mask_acc = acc_view >= thr_cells

# Offsets per analizzare i vicini secondo lo schema D8.
neighbor_offsets = [
    (-1, -1), (-1, 0), (-1, 1),
    (0, -1),           (0, 1),
    (1, -1),  (1, 0),  (1, 1),
]

# Mapping tra offset e codici di direzione del flusso D8.
d8_from_offset = {
    (-1, -1): 32,
    (-1, 0): 64,
    (-1, 1): 128,
    (0, -1): 16,
    (0, 1): 1,
    (1, -1): 8,
    (1, 0): 4,
    (1, 1): 2,
}

# Mapping inverso dai codici D8 all'offset.
d8_to_offset = {
    1: (0, 1),
    2: (1, 1),
    4: (1, 0),
    8: (1, -1),
    16: (0, -1),
    32: (-1, -1),
    64: (-1, 0),
    128: (-1, 1),
}

selected_coords_set = set()
selected_coords_list = []
# Mappa branch_id -> lista di pixel selezionati (per analisi e debug).
branch_selections = {}

def _add_selected_coord(rc):
    """Aggiunge una cella alla lista dei punti selezionati se valida."""
    if rc is None:
        return False
    r, c = map(int, rc)
    if not (0 <= r < H and 0 <= c < W):
        return False
    if catch_view is not None and not catch_view[r, c]:
        return False
    if not mask_acc[r, c]:
        return False

    key = (r, c)
    try:
        exclusion_radius = float(MAIN_SEED_EXCLUSION_RADIUS_METERS)
    except (TypeError, ValueError):
        exclusion_radius = 0.0
    if exclusion_radius > 0 and np.isfinite(exclusion_radius):
        main_seed_rc = (int(main_seed_row), int(main_seed_col))
        if key != main_seed_rc:
            dr_m = (r - main_seed_rc[0]) * cell_size_y
            dc_m = (c - main_seed_rc[1]) * cell_size_x
            distance = math.hypot(dr_m, dc_m)
            if distance <= exclusion_radius:
                return False

    if key in selected_coords_set:
        return False

    selected_coords_set.add(key)
    selected_coords_list.append(key)
    return True

unique_branch_ids = np.unique(branches_raster)
unique_branch_ids = unique_branch_ids[unique_branch_ids > 0]

# Per ciascun ramo seleziona pixel significativi lungo la direzione del flusso.
for branch_id in unique_branch_ids:
    branch_mask = branches_raster == branch_id
    candidate_indices = np.argwhere(branch_mask)
    if candidate_indices.size == 0:
        continue

    # Filtra i pixel del ramo che soddisfano le condizioni sul bacino e sull'accumulo.
    filtered_coords = []
    for r, c in candidate_indices:
        if catch_view is not None and not catch_view[r, c]:
            continue
        if not mask_acc[r, c]:
            continue
        filtered_coords.append((int(r), int(c)))

    if not filtered_coords:
        continue

    filtered_coords.sort(key=lambda rc: acc_view[rc])
    n_coords = len(filtered_coords)

    # Distanze cumulative lungo il ramo (ordinate da monte a valle)
    cumulative_distances = [0.0]
    for idx in range(1, n_coords):
        r0, c0 = filtered_coords[idx - 1]
        r1, c1 = filtered_coords[idx]
        dr = (r1 - r0) * cell_size_y
        dc = (c1 - c0) * cell_size_x
        step_distance = math.hypot(dr, dc)
        cumulative_distances.append(cumulative_distances[-1] + step_distance)

    branch_length = cumulative_distances[-1] if cumulative_distances else 0.0
    try:
        min_branch_length = float(MIN_BRANCH_LENGTH_FOR_INTERMEDIATE_METERS)
    except (TypeError, ValueError):
        min_branch_length = 0.0
    if not np.isfinite(min_branch_length):
        min_branch_length = 0.0
    min_branch_length = max(0.0, min_branch_length)
    allow_intermediate_selection = branch_length >= min_branch_length

    branch_selected = []
    used_indices = set()
    
    mode = (PIXEL_PLACEMENT_MODE or "").strip().lower()

    if mode == "spacing":
        # Nel metodo spacing si parte dal pixel di valle.
        if _add_selected_coord(filtered_coords[-1]):
            branch_selected.append(filtered_coords[-1])
            used_indices.add(n_coords - 1)
    else:
        # Seleziona sempre l'estremo a monte del ramo.
        if _add_selected_coord(filtered_coords[0]):
            branch_selected.append(filtered_coords[0])
            used_indices.add(0)

        # Seleziona sempre l'estremo a valle del ramo.
        if _add_selected_coord(filtered_coords[-1]):
            branch_selected.append(filtered_coords[-1])
            used_indices.add(n_coords - 1)

    # Numero massimo di slot disponibili per eventuali punti intermedi.
    available_slots = max(0, n_coords - 2)

    if (
        n_coords > 2
        and available_slots > 0
        and allow_intermediate_selection
    ):
        # Target distances esprime le posizioni desiderate lungo il ramo.
        target_distances = []

        if mode == "spacing":
            try:
                spacing_m = float(INTERMEDIATE_SPACING_METERS)
            except (TypeError, ValueError):
                spacing_m = 0.0
            if spacing_m > 0 and cumulative_distances[-1] > 0:
                n_targets = int(cumulative_distances[-1] // spacing_m)
                max_intermediate = None
                if MAX_INTERMEDIATE_PIXELS is not None:
                    try:
                        max_intermediate = int(MAX_INTERMEDIATE_PIXELS)
                    except (TypeError, ValueError):
                        max_intermediate = None
                if max_intermediate is not None:
                    n_targets = min(n_targets, max_intermediate)
                n_targets = min(n_targets, available_slots)
                target_distances = [spacing_m * i for i in range(1, n_targets + 1)]
        else:
            max_intermediate = None
            if MAX_INTERMEDIATE_PIXELS is not None:
                try:
                    max_intermediate = int(MAX_INTERMEDIATE_PIXELS)
                except (TypeError, ValueError):
                    max_intermediate = None
            if max_intermediate is not None:
                n_intermediate = min(max_intermediate, available_slots)
            else:
                n_intermediate = max(1, math.ceil(n_coords / 30))
                n_intermediate = min(n_intermediate, available_slots)

            if n_intermediate > 0 and cumulative_distances[-1] > 0:
                target_distances = np.linspace(
                    0, cumulative_distances[-1], n_intermediate + 2
                )[1:-1]

        for target_distance in target_distances:
            if mode == "spacing":
                distance_from_downstream = [
                    branch_length - cumulative_distances[i] for i in range(n_coords)
                ]
                available_indices = [
                    i
                    for i in range(0, n_coords - 1)
                    if i not in used_indices and distance_from_downstream[i] >= target_distance
                ]
                if not available_indices:
                    break
                closest_idx = min(
                    available_indices,
                    key=lambda i: abs(distance_from_downstream[i] - target_distance),
                )
            else:
                available_indices = [
                    i for i in range(1, n_coords - 1) if i not in used_indices
                ]
                if not available_indices:
                    break
                closest_idx = min(
                    available_indices,
                    key=lambda i: abs(cumulative_distances[i] - target_distance),
                )

            coord = filtered_coords[closest_idx]
            if _add_selected_coord(coord):
                branch_selected.append(coord)
            used_indices.add(closest_idx)
        
    if mode == "spacing":
        # Quando si esaurisce il ramo seleziona l'ultimo pixel a monte.
        if _add_selected_coord(filtered_coords[0]):
            branch_selected.append(filtered_coords[0])
            used_indices.add(0)

    if branch_selected:
        branch_selected.sort(key=lambda rc: acc_view[rc])
        branch_selections[int(branch_id)] = branch_selected

total_selected = sum(len(v) for v in branch_selections.values())
print(f"Selezionati {total_selected} pixel su {len(branch_selections)} rami.")

selected_coords_list.sort(key=lambda rc: (-acc_view[rc], rc[0], rc[1]))
# Ordina i pixel per importanza (accumulo decrescente e coordinate stabili).

# Mask dei canali, utile per gli snap e per vincolare la ricerca dei massimi.
channel_mask = branches_raster.astype(bool)
snap_mask = channel_mask & (acc_view >= SNAP_ACCUMULATION_THRESHOLD)
# Precalcola gli indici dei pixel che appartengono alla rete (e al bacino, se presente).
channel_indices = np.argwhere(snap_mask)
if catch_view is not None and channel_indices.size:
    channel_indices = channel_indices[
        catch_view[channel_indices[:, 0], channel_indices[:, 1]]
    ]
# Lista di dizionari con le informazioni principali di ciascun pixel selezionato.
selected_points = []

def _build_selected_mask(points):
    """Costruisce una mask booleana partendo dalla lista di punti selezionati."""
    mask = np.zeros_like(branches_raster, dtype=bool)
    for pt in points:
        r, c = pt.get("snapped_index", (None, None))
        if r is None or c is None:
            continue
        if 0 <= r < H and 0 <= c < W:
            mask[r, c] = True
    return mask


def _local_max_index(r, c, acc_arr, mask=None, max_radius=5):
    """Trova l'indice (row, col) della cella con accumulo massimo vicina."""
    h, w = acc_arr.shape
    if mask is not None:
        mask = mask.astype(bool)
    r = int(r)
    c = int(c)
    best_rc = (r, c)
    if 0 <= r < h and 0 <= c < w:
        best_val = acc_arr[r, c]
    else:
        best_val = -np.inf
    for radius in range(max_radius + 1):
        r0 = max(0, r - radius)
        r1 = min(h, r + radius + 1)
        c0 = max(0, c - radius)
        c1 = min(w, c + radius + 1)
        window = acc_arr[r0:r1, c0:c1]
        if window.size == 0:
            continue
        if mask is not None:
            mask_window = mask[r0:r1, c0:c1]
            if not mask_window.any():
                continue
            masked_vals = np.where(mask_window, window, -np.inf)
            idx = np.argmax(masked_vals)
            if np.isneginf(masked_vals.flat[idx]):
                continue
        else:
            idx = np.argmax(window)
        wr, wc = np.unravel_index(idx, window.shape)
        candidate_val = window[wr, wc]
        if candidate_val > best_val:
            best_val = candidate_val
            best_rc = (r0 + wr, c0 + wc)
        if mask is not None and mask_window[wr, wc]:
            return best_rc
    return best_rc
def _nearest_channel_index(r, c, channel_cells, max_radius=None):
    """Restituisce il pixel della rete più vicino alle coordinate indicate."""

    if channel_cells is None:
        return None

    channel_cells = np.asarray(channel_cells)
    if channel_cells.size == 0 or channel_cells.ndim != 2 or channel_cells.shape[1] != 2:
        return None

    target = np.asarray([int(r), int(c)], dtype=np.int64)
    deltas = channel_cells.astype(np.int64) - target
    dist2 = np.sum(deltas * deltas, axis=1)
    if dist2.size == 0:
        return None

    nearest_idx = int(np.argmin(dist2))
    if max_radius is not None and dist2[nearest_idx] > max_radius ** 2:
        return None

    nearest_r, nearest_c = channel_cells[nearest_idx]
    return int(nearest_r), int(nearest_c)

# Costruisce il set di punti selezionati con coordinate e informazioni di snap.
for rr, cc in selected_coords_list:
    point = {
        "row": int(rr),
        "col": int(cc),
        "original_index": (int(rr), int(cc)),
    }
    x, y = rasterio.transform.xy(grid.affine, rr, cc, offset="center")
    point["initial_coord"] = (float(x), float(y))

    snapped_xy = None
    if channel_mask.any():
        try:
            snapped_xy = grid.snap_to_mask(channel_mask, (x, y))
        except Exception:
            snapped_xy = None
    if snapped_xy is not None:
        xsnap, ysnap = snapped_xy
        try:
            rsnap_float, csnap_float = rowcol(grid.affine, xsnap, ysnap)
            rsnap, csnap = int(rsnap_float), int(csnap_float)
        except Exception:
            rsnap, csnap = int(rr), int(cc)
    else:
        rsnap, csnap = int(rr), int(cc)
        xsnap, ysnap = x, y

    if not (0 <= rsnap < H and 0 <= csnap < W) or not channel_mask[rsnap, csnap]:
        rsnap, csnap = _local_max_index(rr, cc, acc_view, mask=channel_mask)
        xsnap, ysnap = rasterio.transform.xy(
            grid.affine, rsnap, csnap, offset="center"
        )

    point["snapped_index"] = (int(rsnap), int(csnap))
    point["snapped_coord"] = (float(xsnap), float(ysnap))
    selected_points.append(point)

# Costruisce il set di punti selezionati corrispondenti alle stazioni sulla rete.
stations_on_network = []
stations_outside_network = []

if station_metadata:
    total_stations = len(station_metadata)
    for record in station_metadata:
        lon = record.get("lon")
        lat = record.get("lat")
        if lon is None or lat is None:
            continue
        
        try:
            rr_float, cc_float = rowcol(grid.affine, lon, lat)
        except Exception:
            continue
        
        rr = int(rr_float)
        cc = int(cc_float)
        if not (0 <= rr < H and 0 <= cc < W):
            continue
        
        if catch_view is not None and not catch_view[rr, cc]:
            stations_outside_network.append(record)
            continue
        
        rsnap = csnap = None
        xsnap = ysnap = None
        

        snapped_xy = None
        if snap_mask.any():
            try:
                snapped_xy = grid.snap_to_mask(snap_mask, (lon, lat))
            except Exception:
                snapped_xy = None

        if snapped_xy is not None:
            xsnap, ysnap = map(float, snapped_xy)
            try:
                rsnap_float, csnap_float = rowcol(grid.affine, xsnap, ysnap)
                rsnap, csnap = int(rsnap_float), int(csnap_float)
            except Exception:
                rsnap = csnap = None

        if rsnap is None or csnap is None:
            rsnap, csnap = rr, cc
            xsnap, ysnap = map(float, (lon, lat))

        if (
            not (0 <= rsnap < H and 0 <= csnap < W)
            or not snap_mask[rsnap, csnap]
            or (catch_view is not None and not catch_view[rsnap, csnap])
        ):
            nearest_rc = _nearest_channel_index(rr, cc, channel_indices)
            if nearest_rc is None:
                nearest_rc = _local_max_index(
                    rr, cc, acc_view, mask=snap_mask, max_radius=10
                )
            rsnap, csnap = nearest_rc
            xsnap, ysnap = rasterio.transform.xy(
                grid.affine, rsnap, csnap, offset="center"
            )
            
        if not (0 <= rsnap < H and 0 <= csnap < W):
            continue
        if not snap_mask[rsnap, csnap]:
            stations_outside_network.append(record)
            continue
        if catch_view is not None and not catch_view[rsnap, csnap]:
            stations_outside_network.append(record)
            continue

        point = {
            "row": int(rr),
            "col": int(cc),
            "original_index": (int(rr), int(cc)),
            "initial_coord": (float(lon), float(lat)),
            "snapped_index": (int(rsnap), int(csnap)),
            "snapped_coord": (float(xsnap), float(ysnap)),
            "station_id": record.get("station_id"),
            "station_record": record,
        }
        selected_points.append(point)
        stations_on_network.append(record)

    print(
        f"Stazioni selezionate sulla rete: {len(stations_on_network)} su {total_stations} totali."
    )
    if stations_outside_network:
        print(
            f"Stazioni escluse perché fuori dalla rete: {len(stations_outside_network)}"
        )
else:
    print("Nessun metadata stazioni disponibile: nessuna stazione selezionata.")

# Rimuovi eventuali duplicati basati sulle coordinate "snappate" e
# verifica che tutti i pixel selezionati abbiano coordinate distinte.
unique_points = []
seen_snapped_indices = set()
duplicate_count = 0

for point in selected_points:
    snapped_key = point.get("snapped_index")
    if snapped_key in seen_snapped_indices:
        duplicate_count += 1
        continue
    seen_snapped_indices.add(snapped_key)
    unique_points.append(point)

if duplicate_count:
    print(f"Rimossi {duplicate_count} pixel duplicati basati sulle coordinate snappate.")

selected_points = unique_points

# Controllo finale: assicurati che tutte le coordinate siano differenti.
assert len(selected_points) == len(seen_snapped_indices), (
    "Sono presenti pixel duplicati nelle coordinate selezionate.")

# --- Tabella dei pixel selezionati ---
def _format_float_or_empty(value, precision=6):
    if value is None:
        return ""
    try:
        if not math.isfinite(value):
            return ""
    except TypeError:
        return ""
    return f"{value:.{precision}f}"


def _format_int_or_empty(value):
    if value is None:
        return ""
    try:
        return str(int(value))
    except (TypeError, ValueError):
        return ""

def _format_str_or_empty(value):
    if value is None:
        return ""
    value = str(value).strip()
    return value

pixel_table_rows = []
main_seed_rc = (int(main_seed_row), int(main_seed_col))

for idx, point in enumerate(selected_points, start=1):
    rsnap, csnap = point.get("snapped_index", (None, None))
    xsnap, ysnap = point.get("snapped_coord", (None, None))
    station_id = point.get("station_id")
    distance_m = None
    if rsnap is not None and csnap is not None:
        try:
            dr_m = (int(rsnap) - main_seed_rc[0]) * PIXEL_SIZE_METERS
            dc_m = (int(csnap) - main_seed_rc[1]) * PIXEL_SIZE_METERS
            distance_m = math.hypot(dr_m, dc_m)
        except (TypeError, ValueError):
            distance_m = None
            
    pixel_table_rows.append(
        {
            "id": idx,
            "station_id": station_id,
            "row": rsnap,
            "col": csnap,
            "x": xsnap,
            "y": ysnap,
            "distance_m": distance_m,
        }
    )

if pixel_table_rows:
    header = ("ID", "Station_ID", "Row", "Col", "X", "Y", "Distance_m")
    col_widths = [max(len(h), 5) for h in header]
    
    def _row_values(row):
        return (
            _format_int_or_empty(row["id"]),
            _format_int_or_empty(row["row"]),
            _format_int_or_empty(row["col"]),
            _format_str_or_empty(row.get("station_id")),
            _format_float_or_empty(row["x"]),
            _format_float_or_empty(row["y"]),
            _format_float_or_empty(row.get("distance_m")),
        )

    for row in pixel_table_rows:
        values = _row_values(row)
        for idx_col, value in enumerate(values):
            col_widths[idx_col] = max(col_widths[idx_col], len(str(value)))

    def _print_row(values):
        formatted = [str(val).rjust(col_widths[i]) for i, val in enumerate(values)]
        print(" ".join(formatted))

    print("\nTabella pixel selezionati (coordinate snappate):")
    _print_row(header)
    print(" ".join("-" * width for width in col_widths))
    for row in pixel_table_rows:
        _print_row(_row_values(row))

    if PIXEL_TABLE_OUTPUT:
        try:
            with PIXEL_TABLE_OUTPUT.open("w", newline="", encoding="utf-8") as fp:
                writer = csv.writer(fp)
                writer.writerow([
                    "pixel_id",
                    "station_id",
                    "row",
                    "col",
                    "x",
                    "y",
                    "distance_m",
                ])
                for row in pixel_table_rows:
                    writer.writerow(
                        [
                            row["id"],
                            row.get("station_id"),
                            row["row"],
                            row["col"],
                            row["x"],
                            row["y"],
                            row.get("distance_m"),
                        ]
                    )
            print(
                f"Tabella pixel salvata in {PIXEL_TABLE_OUTPUT.resolve()}"
            )
        except OSError as exc:
            print(
                f"Impossibile salvare la tabella pixel su {PIXEL_TABLE_OUTPUT}: {exc}"
            )
else:
    print("Nessun pixel selezionato: nessuna tabella da esportare.")

#%% Area contribuente dei pixel estratti (catchments)

xmin, ymin, xmax, ymax = grid.bbox

catchments = []   # lista di maschere boolean (una per punto)
areas = []        # area m² (o unità del tuo CRS) di ogni catchment
labels = np.zeros_like(fdir_view, dtype=np.int32)  # raster etichettato (facoltativo)
expected_areas = []
catchment_points = []
failed_points = []
catchment_ids = []

main_mask = grid.view(catch).astype(bool)   # maschera del bacino principale nella view clippata
catchments    = [main_mask] + catchments
catchment_ids = ["catchment_main"] + catchment_ids

os.makedirs("catchments_tif", exist_ok=True)

for point in selected_points:
    rsnap, csnap = point["snapped_index"]
    x_snap, y_snap = point["snapped_coord"]
    
    

    if not (0 <= rsnap < H and 0 <= csnap < W):
        failed_points.append({"point": point, "reason": "indice fuori dai limiti"})
        continue

    expected_area = acc_view[rsnap, csnap] * cell_area_m2
    if expected_area <= 0:
        failed_points.append({"point": point, "reason": "accumulation nulla"})
        continue

    def _compute_catchment(row, col):
        """Estrae l'area contribuente a partire da una cella della rete."""

        row = int(row)
        col = int(col)
        if not (0 <= row < H and 0 <= col < W):
            return None
        if catch_view is not None and not catch_view[row, col]:
            return None

        try:
            mask_local = grid.catchment(
                x=col,
                y=row,
                fdir=fdir_view,
                xytype="index",
            )
        except Exception as exc:
            print(
                f"Errore nell'estrazione del sottobacino per l'indice "
                f"({row}, {col}): {exc}"
            )
            return None

        if mask_local is None:
            return None

        mask_local = np.asarray(mask_local, dtype=bool)
        if catch_view is not None:
            mask_local &= catch_view

        if not mask_local.any():
            return None

        return mask_local

    mask = _compute_catchment(rsnap, csnap)

    if mask is None:
        alt_r, alt_c = _local_max_index(
            rsnap, csnap, acc_view, mask=channel_mask, max_radius=10
        )
        
        if (alt_r, alt_c) != (rsnap, csnap):
            rsnap, csnap = int(alt_r), int(alt_c)
            x_snap, y_snap = rasterio.transform.xy(
                grid.affine, rsnap, csnap, offset='center'
            )
            expected_area = acc_view[rsnap, csnap] * cell_area_m2
            if expected_area > 0:
                mask = _compute_catchment(rsnap, csnap)

    if mask is None:
        x_orig, y_orig = point.get("initial_coord", (None, None))
        r_orig, c_orig = point["row"], point["col"]
        if (
            x_orig is not None
            and y_orig is not None
            and 0 <= r_orig < H
            and 0 <= c_orig < W
        ):
            expected_area_orig = acc_view[r_orig, c_orig] * cell_area_m2
            if expected_area_orig > 0:
                candidate_mask = _compute_catchment(r_orig, c_orig)
                if candidate_mask is not None:
                    mask = candidate_mask
                    rsnap, csnap = int(r_orig), int(c_orig)
                    x_snap, y_snap = float(x_orig), float(y_orig)
                    expected_area = expected_area_orig

    if mask is None:
        point["catchment_found"] = False
        failed_points.append({"point": point, "reason": "catchment non trovato"})
        continue
    
    point["catchment_found"] = True
    point["snapped_index"] = (int(rsnap), int(csnap))
    point["snapped_coord"] = (float(x_snap), float(y_snap))

    actual_area = mask.sum() * cell_area_m2
    deviation = (
        abs(actual_area - expected_area) / expected_area if expected_area else np.inf
    )
    if deviation > 0.2:
        print(
            f"Avviso: deviazione area {deviation*100:.1f}% per il seed in {x_snap:.2f}, {y_snap:.2f}"
        )

    label_id = len(catchment_ids)
    station_identifier = str(point.get("station_id") or "").strip()
    if not station_identifier:
        station_identifier = f"{label_id:03d}"
    station_identifier = station_identifier.replace(" ", "_")
    catchment_id = f"catchment_{station_identifier}"
        
    catchments.append(mask)
    catchment_ids.append(catchment_id)
    areas.append(actual_area)
    expected_areas.append(expected_area)

    point["expected_area"] = float(expected_area)
    point["catchment_area"] = float(actual_area)
    point["deviation"] = float(deviation)
    catchment_points.append(point.copy())

    labels[mask & (labels == 0)] = label_id

selected_mask = _build_selected_mask(catchment_points)
pixels_selected = selected_mask.astype(np.uint8)

#%%%plot pixel sulla rete
sns.set_palette('husl')
fig, ax = plt.subplots(figsize=(8.5, 6.5))

plt.xlim(grid.bbox[0], grid.bbox[2])
plt.ylim(grid.bbox[1], grid.bbox[3])
ax.set_aspect('equal')

for branch in branches['features']:
    line = np.asarray(branch['geometry']['coordinates'])
    plt.plot(line[:, 0], line[:, 1])

station_points = [
    pt for pt in catchment_points if str(pt.get("station_id") or "").strip()
]
intermediate_points = [
    pt for pt in catchment_points if not str(pt.get("station_id") or "").strip()
]

if intermediate_points:
    ax.scatter(
        [pt["snapped_coord"][0] for pt in intermediate_points],
        [pt["snapped_coord"][1] for pt in intermediate_points],
        s=18,
        c="red",
        edgecolors="k",
        linewidths=0.3,
        zorder=5,
        label="Pixel intermedi",
    )
if station_points:
    ax.scatter(
        [pt["snapped_coord"][0] for pt in station_points],
        [pt["snapped_coord"][1] for pt in station_points],
        s=18,
        c="orange",
        edgecolors="k",
        linewidths=0.3,
        zorder=6,
        label="Pixel stazioni",
    )
_ = plt.title(
    f" Selected pixels on channel network (>{BRANCH_ACCUMULATION_THRESHOLD} accumulation)",
    size=14,
)

#%%%controllo pixels
count_ones = np.sum(branches_raster > 0)
selected_count = pixels_selected.sum()
print("Pixel canali", count_ones)
print("Numero confluences:", selected_count)

check = pixels_selected.astype(bool) & (branches_raster > 0)
check_ones = np.sum(check)
print("confluences sulla rete", check_ones)

if check_ones == selected_count:
    print("Pixel Trovati")
else:
    print("Discrepanze pixel rete")

if failed_points:
    print(f"Catchment non estratti: {len(failed_points)}")
    for item in failed_points:
        coord = item["point"].get("initial_coord", (np.nan, np.nan))
        print(f" - seed {coord} -> {item['reason']}")

xs = np.asarray([pt["snapped_coord"][0] for pt in catchment_points])
ys = np.asarray([pt["snapped_coord"][1] for pt in catchment_points])

print(f"Catchment estratti: {len(catchments) - 1}")
if expected_areas:
    areas_arr = np.asarray(areas)
    expected_arr = np.asarray(expected_areas)
    deviation_pct = np.where(expected_arr > 0,
                             np.abs(areas_arr - expected_arr) / expected_arr * 100,
                             np.nan)
    print(f"Deviazione media area rispetto ad accumulation: {np.nanmean(deviation_pct):.2f}%")
#%%% Plot Aree contribuenti

def plot_catchment_i(catchment_id,
                     catchments,        # lista di mask boolean (come creato prima)
                     xs, ys,            # coordinate dei seed (solo sottobacini)
                     catch,             # mask booleana del bacino principale (stessa view)
                     transform_view,    # grid.affine della view clippata
                     dem_view=None,     # opzionale: DEM nella view (stessa shape)
                     main_seed=None,    # opzionale: (x0, y0) seed del bacino principale
                     catchment_ids=None # opzionale: lista ID catchment per il titolo
                     ):
    """Visualizza il catchment richiesto sovrapposto al bacino principale.

    ``catchment_id`` può essere:

    * l'ID testuale del catchment (es. ``"catchment_001"``);
    * un indice intero della lista ``catchment_ids`` (es. ``1``);
    * una stringa contenente un intero (es. ``"1"``).
    """
    
    if not catchments:
        raise ValueError("Nessun catchment disponibile per il plotting")
    if not catchment_ids:
        raise ValueError("È necessario fornire gli ID dei catchment per il plotting")

    index = None

    if isinstance(catchment_id, int):
        index = catchment_id
    else:
        cid_str = str(catchment_id)
        if cid_str.isdigit():
            index = int(cid_str)
        else:
            try:
                index = catchment_ids.index(cid_str)
            except ValueError as exc:
                raise KeyError(f"Catchment ID '{catchment_id}' non trovato") from exc

    if index is None or not (0 <= index < len(catchment_ids)):
        raise IndexError(
            f"Indice catchment {catchment_id!r} fuori dall'intervallo disponibile"
        )

    mask_i = catchments[index]          # boolean array (H, W)
    h, w = mask_i.shape
    xmin, ymin, xmax, ymax = rasterio.transform.array_bounds(h, w, 
                                                             transform_view)

    fig, ax = plt.subplots(figsize=(7, 7))

    # 1) base DEM (opzionale)
    if dem_view is not None and dem_view.shape == mask_i.shape:
        ax.imshow(dem_view, extent=(xmin, xmax, ymin, ymax), origin="upper", 
                  cmap="gray")

    # 2) bacino principale in grigio trasparente
    ax.imshow(np.where(catch, 1, np.nan), extent=(xmin, xmax, ymin, ymax),
              origin="upper", alpha=0.25)
    
    # 4) sottobacino i-esimo più marcato
    ax.imshow(np.where(mask_i, 1, np.nan), extent=(xmin, xmax, ymin, ymax),
              origin="upper")
    
    # 3) rete di drenaggio
    for branch in branches['features']:
            line = np.asarray(branch['geometry']['coordinates'])
            plt.plot(line[:, 0], line[:, 1], alpha=0.5)
 

    # ) punti seed
    # seed del sottobacino i-esimo
    if main_seed is not None:
        ax.scatter(
            [main_seed[0]],
            [main_seed[1]],
            s=40,
            marker='^',
            edgecolor='k',
            linewidth=0.8,
            label="sezione principale",
        )
    
    catchment_label = catchment_ids[index]
    # punti seed dei sottobacini
    if xs is not None and ys is not None and len(xs) and len(ys):
        ax.scatter(
            xs,
            ys,
            s=20,
            c='red',
            edgecolor='k',
            linewidths=0.3,
            label='sezioni estratte',
        )

    # evidenzia il seed del sottobacino corrente
    highlighted_seed = None
    if index == 0:
        highlighted_seed = main_seed
    else:
        seed_index = index - 1
        if (
            xs is not None
            and ys is not None
            and 0 <= seed_index < len(xs)
        ):
            highlighted_seed = (xs[seed_index], ys[seed_index])

    if highlighted_seed is not None:
        ax.scatter(
            [highlighted_seed[0]],
            [highlighted_seed[1]],
            s=45,
            marker='o',
            facecolor='yellow',
            edgecolor='k',
            linewidth=0.6,
            label=f'sezione {catchment_label}',
        )

    ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)
    ax.set_xlabel("x"); ax.set_ylabel("y")
    ax.set_title(f"Area contribuente {catchment_label} su bacino principale")
    add_scale_bar(ax, transform_view, location='lower right')
    ax.legend(loc="best")
    plt.show()
    
    
catchment_selection = "6"  # può essere indice (es. 1) o ID (es. "catchment_001")
catchment_id_to_plot = catchment_selection if catchment_ids else None

if catchment_id_to_plot is not None:
    plot_catchment_i(
        catchment_id_to_plot,
        catchments=catchments,
        xs=xs,
        ys=ys,
        catch=grid.view(catch),
        transform_view=grid.affine,
        main_seed=(main_seed_x, main_seed_y),
        catchment_ids=catchment_ids,
    )
else:
    print("Nessun catchment valido da plottare.")

#%% Serie di precipitazione media per catchment
if not RUN_KRIGING:
    print("Kriging disabilitato; eseguita solo l'estrazione della rete.")
elif not catchments:
    print("Nessun catchment disponibile per il calcolo delle precipitazioni medie.")
else:
    crs_obj = profile.get("crs") if isinstance(profile, dict) else None
    dtm_crs = None
    if crs_obj is not None:
        dtm_crs = crs_obj.to_string() if hasattr(crs_obj, "to_string") else str(crs_obj)

    def _normalize_requested_ids(raw_ids):
        if raw_ids is None:
            return []
        if isinstance(raw_ids, (list, tuple, set)):
            items = [str(item) for item in raw_ids]
        else:
            items = [part for part in str(raw_ids).replace(";", ",").split(",")]
        normalized = []
        for item in items:
    
            clean = item.strip()
            if not clean:
                continue
            normalized.append(clean)
        return normalized

    requested_ids = _normalize_requested_ids(KRIGING_CATCHMENT_IDS)

    if not requested_ids and PROMPT_KRIGING_CATCHMENTS:
        try:
            user_input = input(
                "Inserisci gli ID dei catchment per il kriging (separati da virgola) "
                "oppure premi invio per tutti:"
            )
        except EOFError:
            user_input = ""
        user_input = user_input.strip()
        if user_input and user_input.lower() not in {"tutti", "all"}:
            requested_ids = _normalize_requested_ids(user_input)
        else:
            requested_ids = []

    catchment_map = dict(zip(catchment_ids, catchments))
    selected_ids = list(catchment_ids)
    selected_masks = list(catchments)

    if requested_ids:
        missing = [cid for cid in requested_ids if cid not in catchment_map]
        if missing:
            print(
                "Catchment non presenti tra quelli estratti: "
                + ", ".join(missing)
            )
        selected_ids = [cid for cid in requested_ids if cid in catchment_map]
    else:
        selected_ids = list(catchment_ids)
        if selected_ids:
            print(
                "Nessun ID specificato: verranno elaborati tutti i catchment disponibili."
            )
    
    selected_masks = [catchment_map[cid] for cid in selected_ids]

    if not selected_ids:
        print(
            "Nessun catchment selezionato per il kriging; eseguita solo l'estrazione della rete."
        )
    else:
        print(
            "Avvio calcolo serie di precipitazione media per catchment tramite kriging "
            f"({len(selected_ids)} catchment)."
        )
        try:
            precip_table = compute_catchment_precipitation_series(
                catchment_masks=selected_masks,
                transform=grid.affine,
                catchment_ids=selected_ids,
                csv_dir=PRECIP_DATA_DIR,
                station_meta=PRECIP_STATION_META_PATH,
                dtm_path=Path(dem_path),
                dtm_crs=dtm_crs,
                start_date=PRECIP_DATE_START,
                end_date=PRECIP_DATE_END,
                buffer_distance=PRECIP_BUFFER_METERS,
                min_stations=PRECIP_MIN_STATIONS,
                max_stations=PRECIP_MAX_STATIONS,
                aggregation_factor=AGGREGATION_FACTOR,
                precipitation_threshold=KRIGING_PRECIPITATION_THRESHOLD,
            )
            if not precip_table.empty:
                precip_table.to_csv(CATCHMENT_SERIES_OUTPUT, index=False)
                print(
                    "Tabella precipitazioni media catchment salvata in "
                    f"{CATCHMENT_SERIES_OUTPUT.resolve()}"
                )
                try:
                    output_file = run_model(
                        CATCHMENT_SERIES_OUTPUT,
                        output_dir=MODEL_OUTPUT_DIR,
                        precipitation_thresholds=MODEL_PRECIPITATION_THRESHOLDS,
                    )
                    if output_file:
                        if isinstance(output_file, list):
                            for model_output in output_file:
                                print(
                                    "Output Model salvato in "
                                    f"{model_output.resolve()}"
                                )
                        else:
                            print(
                                "Output Model salvato in "
                                f"{output_file.resolve()}"
                            )
                except Exception as exc:
                    print(f"Errore durante l'esecuzione del Model: {exc}")
            else:
                print(
                    "Nessuna serie di precipitazione calcolata: verificare dati di input o catchments."
                )
        except Exception as exc:
            print(f"Errore durante il calcolo delle serie di precipitazione: {exc}")
            
        plot_dates = _build_plot_date_list(
            date_string=PLOT_PRECIPITATION_DATE,
            date_range=PLOT_PRECIPITATION_RANGE,
        )

        if PROMPT_PLOT_PRECIPITATION:
            try:
                user_plot_date = input(
                    "Inserisci una data singola (YYYY-MM-DD) o un intervallo "
                    "YYYY-MM-DD:YYYY-MM-DD per plottare le precipitazioni sul bacino "
                )
            except EOFError:
                user_plot_date = ""
            user_plot_date = user_plot_date.strip()
            
            if user_plot_date:
                if ":" in user_plot_date:
                    start_str, end_str = user_plot_date.split(":", 1)
                    plot_dates = _build_plot_date_list(
                        date_range=(start_str.strip(), end_str.strip())
                    )
                else:
                    plot_dates = _build_plot_date_list(date_string=user_plot_date)

        plot_dates = list(dict.fromkeys(plot_dates))
        
        agg_pixel_size_m = (
            PIXEL_SIZE_METERS * AGGREGATION_FACTOR
            if AGGREGATION_FACTOR and AGGREGATION_FACTOR > 1
            else PIXEL_SIZE_METERS
        )

        for plot_date in plot_dates:
            plot_date_ts = pd.to_datetime(plot_date, errors="coerce")
            plot_date_label = (
                plot_date_ts.strftime("%Y-%m-%d")
                if not pd.isna(plot_date_ts)
                else str(plot_date)
            )
            try:
                basin_grid, agg_transform, station_plot_data = krige_basin_precipitation_grid(
                    date=plot_date_label,
                    catchment_masks=selected_masks,
                    transform=grid.affine,
                    csv_dir=PRECIP_DATA_DIR,
                    station_meta=PRECIP_STATION_META_PATH,
                    dtm_path=Path(dem_path),
                    dtm_crs=dtm_crs,
                    buffer_distance=PRECIP_BUFFER_METERS,
                    max_stations=PRECIP_MAX_STATIONS,
                    aggregation_factor=AGGREGATION_FACTOR,
                )
                plot_precipitation_map(
                   plot_date_label,
                    basin_grid,
                    agg_transform,
                    station_plot_data,
                    pixel_size_meters=agg_pixel_size_m,
                )
            except Exception as exc:
                print(
                    "Impossibile generare il plot delle precipitazioni per "
                    f"{plot_date_label}: {exc}"
                )
                
